#!/usr/bin/env python3
"""
‚õß IntelligentCache - Cache Intelligent avec R√©troinjection ‚õß

Syst√®me de cache intelligent qui utilise l'analyse LLM pour l'adaptation contextuelle
et la r√©troinjection d'apprentissages, sans patterns rigides.
"""

import hashlib
import json
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import OrderedDict

from Core.LLMProviders import LLMProvider
from .intelligent_parser import IntrospectiveMessage


@dataclass
class CacheEntry:
    """Entr√©e du cache intelligent"""
    response_hash: str
    original_message: IntrospectiveMessage
    context_signature: str
    effectiveness_score: float
    adaptation_history: List[Dict[str, Any]]
    last_used: float
    usage_count: int


@dataclass
class ContextSignature:
    """Signature contextuelle pour l'adaptation"""
    entity_id: str
    entity_type: str
    context_features: Dict[str, Any]
    timestamp: float


class IntelligentCache:
    """Cache intelligent avec r√©troinjection d'apprentissages"""
    
    def __init__(self, provider: LLMProvider, max_cache_size: int = 1000):
        self.provider = provider
        self.max_cache_size = max_cache_size
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.learning_patterns: Dict[str, float] = {}  # Patterns d'apprentissage avec scores
        self.context_adaptations: Dict[str, List[Dict]] = {}  # Historique des adaptations
    
    async def get_cached_analysis(self, response: str, entity_id: str, 
                                entity_type: str, context: Optional[Dict[str, Any]] = None) -> Optional[IntrospectiveMessage]:
        """
        R√©cup√®re une analyse en cache avec adaptation contextuelle intelligente
        
        Args:
            response: R√©ponse √† analyser
            entity_id: Identifiant de l'entit√©
            entity_type: Type d'entit√©
            context: Contexte actuel
            
        Returns:
            Message introspectif adapt√© ou None si pas en cache
        """
        # G√©n√©ration du hash de la r√©ponse
        response_hash = self._generate_response_hash(response)
        
        if response_hash in self.cache:
            cache_entry = self.cache[response_hash]
            
            # Mise √† jour des m√©triques d'usage
            cache_entry.last_used = time.time()
            cache_entry.usage_count += 1
            
            # Adaptation contextuelle intelligente
            adapted_message = await self._adapt_cached_result(cache_entry, entity_id, entity_type, context)
            
            # Apprentissage de l'efficacit√© de l'adaptation
            await self._learn_adaptation_effectiveness(cache_entry, adapted_message, context)
            
            print(f"üß† Cache hit: {response_hash[:8]}... (usage: {cache_entry.usage_count})")
            return adapted_message
        
        return None
    
    async def cache_analysis(self, response: str, message: IntrospectiveMessage, 
                           entity_id: str, entity_type: str, 
                           context: Optional[Dict[str, Any]] = None):
        """
        Met en cache une analyse avec signature contextuelle
        
        Args:
            response: R√©ponse analys√©e
            message: Message introspectif
            entity_id: Identifiant de l'entit√©
            entity_type: Type d'entit√©
            context: Contexte de l'analyse
        """
        response_hash = self._generate_response_hash(response)
        context_signature = self._create_context_signature(entity_id, entity_type, context)
        
        # Calcul du score d'efficacit√© initial
        effectiveness_score = self._calculate_effectiveness_score(message, context)
        
        cache_entry = CacheEntry(
            response_hash=response_hash,
            original_message=message,
            context_signature=context_signature,
            effectiveness_score=effectiveness_score,
            adaptation_history=[],
            last_used=time.time(),
            usage_count=1
        )
        
        # Gestion de la taille du cache
        if len(self.cache) >= self.max_cache_size:
            self._evict_least_effective_entry()
        
        self.cache[response_hash] = cache_entry
        print(f"üíæ Cached: {response_hash[:8]}... (effectiveness: {effectiveness_score:.2f})")
    
    async def _adapt_cached_result(self, cache_entry: CacheEntry, entity_id: str, 
                                 entity_type: str, current_context: Optional[Dict[str, Any]]) -> IntrospectiveMessage:
        """
        Adapte un r√©sultat en cache au contexte actuel via analyse LLM
        
        Args:
            cache_entry: Entr√©e du cache
            entity_id: Identifiant de l'entit√© actuelle
            entity_type: Type d'entit√© actuel
            current_context: Contexte actuel
            
        Returns:
            Message introspectif adapt√©
        """
        # Analyse des diff√©rences contextuelles via LLM
        context_diff_analysis = await self._analyze_context_differences(
            cache_entry.original_message.context, current_context, entity_id, entity_type
        )
        
        # Adaptation intelligente bas√©e sur l'analyse LLM
        adapted_message = await self._perform_intelligent_adaptation(
            cache_entry.original_message, context_diff_analysis, current_context
        )
        
        # Enregistrement de l'adaptation
        adaptation_record = {
            "timestamp": time.time(),
            "context_diff_analysis": context_diff_analysis,
            "adaptation_applied": True,
            "original_effectiveness": cache_entry.effectiveness_score
        }
        cache_entry.adaptation_history.append(adaptation_record)
        
        return adapted_message
    
    async def _analyze_context_differences(self, original_context: Optional[Dict[str, Any]], 
                                         current_context: Optional[Dict[str, Any]], 
                                         entity_id: str, entity_type: str) -> Dict[str, Any]:
        """
        Analyse les diff√©rences contextuelles via LLM avec injection de m√©triques
        
        Args:
            original_context: Contexte original
            current_context: Contexte actuel
            entity_id: Identifiant de l'entit√©
            entity_type: Type d'entit√©
            
        Returns:
            Analyse des diff√©rences contextuelles avec m√©triques int√©gr√©es
        """
        # Injection dynamique de m√©triques dans le prompt principal
        metrics_injection = """
√âVALUATION M√âTRIQUES (√† inclure dans la r√©ponse) :
- Complexit√©_contextuelle: score 0-1 bas√© sur le nombre de diff√©rences
- Impact_adaptation: score 0-1 bas√© sur l'impact des diff√©rences
- Confiance_analyse: score 0-1 bas√© sur la clart√© des diff√©rences
"""
        
        prompt = f"""
Analyse les diff√©rences contextuelles entre deux contextes pour l'entit√© {entity_id} ({entity_type}).

Contexte original:
{json.dumps(original_context, ensure_ascii=False, indent=2) if original_context else "Aucun contexte"}

Contexte actuel:
{json.dumps(current_context, ensure_ascii=False, indent=2) if current_context else "Aucun contexte"}

T√ÇCHES √Ä EFFECTUER :
1. Identifie les diff√©rences significatives qui pourraient affecter l'analyse introspective
2. √âvalue la complexit√© contextuelle et l'impact d'adaptation
3. Calcule la confiance dans ton analyse

{metrics_injection}

Retourne un JSON avec cette structure:
{{
    "significant_differences": [
        {{
            "aspect": "description de l'aspect",
            "original_value": "valeur originale",
            "current_value": "valeur actuelle",
            "impact_level": "high|medium|low"
        }}
    ],
    "adaptation_needed": true/false,
    "adaptation_type": "description du type d'adaptation n√©cessaire",
    "metrics": {{
        "complexity_contextuelle": 0.75,
        "impact_adaptation": 0.60,
        "confiance_analyse": 0.85
    }}
}}
"""
        
        try:
            response = await self.provider.generate_response(prompt, temperature=0.1)
            analysis = json.loads(response.content)
            return analysis
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur analyse contextuelle: {e}")
            return {
                "significant_differences": [],
                "adaptation_needed": False,
                "adaptation_type": "none",
                "metrics": {
                    "complexity_contextuelle": 0.0,
                    "impact_adaptation": 0.0,
                    "confiance_analyse": 0.0
                }
            }
    
    async def _perform_intelligent_adaptation(self, original_message: IntrospectiveMessage, 
                                            context_diff_analysis: Dict[str, Any], 
                                            current_context: Optional[Dict[str, Any]]) -> IntrospectiveMessage:
        """
        Effectue l'adaptation intelligente via LLM avec √©valuation int√©gr√©e
        
        Args:
            original_message: Message original
            context_diff_analysis: Analyse des diff√©rences contextuelles
            current_context: Contexte actuel
            
        Returns:
            Message adapt√© avec m√©triques d'adaptation
        """
        if not context_diff_analysis.get("adaptation_needed", False):
            return original_message
        
        # Injection de m√©triques d'adaptation dans le prompt
        adaptation_metrics = """
√âVALUATION D'ADAPTATION (√† inclure dans la r√©ponse) :
- Qualit√©_adaptation: score 0-1 bas√© sur la coh√©rence de l'adaptation
- Fid√©lit√©_original: score 0-1 bas√© sur la pr√©servation du sens original
- Pertinence_contexte: score 0-1 bas√© sur l'ad√©quation au nouveau contexte
"""
        
        prompt = f"""
T√ÇCHES √Ä EFFECTUER :
1. Adapte ce message introspectif au nouveau contexte
2. √âvalue la qualit√© de l'adaptation effectu√©e
3. Calcule les m√©triques de fid√©lit√© et pertinence

Message original:
{json.dumps(original_message.to_dict(), ensure_ascii=False, indent=2)}

Analyse des diff√©rences contextuelles:
{json.dumps(context_diff_analysis, ensure_ascii=False, indent=2)}

Contexte actuel:
{json.dumps(current_context, ensure_ascii=False, indent=2) if current_context else "Aucun contexte"}

{adaptation_metrics}

Adapte le message en conservant sa structure mais en ajustant le contenu selon les diff√©rences contextuelles.
Retourne le message adapt√© avec les m√©triques d'√©valuation au format JSON.
"""
        
        try:
            response = await self.provider.generate_response(prompt, temperature=0.2)
            adapted_data = json.loads(response.content)
            
            # Reconstruction du message adapt√©
            adapted_message = IntrospectiveMessage(
                entity_id=original_message.entity_id,
                entity_type=original_message.entity_type,
                timestamp=time.time(),
                original_response=original_message.original_response,
                thoughts=adapted_data.get("thoughts", original_message.thoughts),
                actions=adapted_data.get("actions", original_message.actions),
                observations=adapted_data.get("observations", original_message.observations),
                decisions=adapted_data.get("decisions", original_message.decisions),
                overall_confidence=adapted_data.get("overall_confidence", original_message.overall_confidence),
                context=current_context
            )
            
            # Stockage des m√©triques d'adaptation dans les m√©tadonn√©es
            if "adaptation_metrics" in adapted_data:
                adapted_message.metadata["adaptation_metrics"] = adapted_data["adaptation_metrics"]
            
            return adapted_message
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur adaptation intelligente: {e}")
            return original_message
    
    def _calculate_effectiveness_score(self, message: IntrospectiveMessage, 
                                     context: Optional[Dict[str, Any]]) -> float:
        """
        Calcule un score d'efficacit√© simple bas√© sur le contenu
        
        Args:
            message: Message introspectif
            context: Contexte de l'analyse
            
        Returns:
            Score d'efficacit√© entre 0.0 et 1.0
        """
        # Score simple bas√© sur la pr√©sence d'√©l√©ments introspectifs
        total_elements = len(message.thoughts) + len(message.actions) + len(message.observations) + len(message.decisions)
        
        if total_elements == 0:
            return 0.1  # Score tr√®s bas si aucun √©l√©ment d√©tect√©
        
        # Score bas√© sur la confiance moyenne des √©l√©ments
        all_elements = message.thoughts + message.actions + message.observations + message.decisions
        if all_elements:
            avg_confidence = sum(elem.confidence for elem in all_elements) / len(all_elements)
            return min(avg_confidence, 0.9)  # Cap √† 0.9 pour √©viter les scores parfaits
        
        return 0.5  # Score par d√©faut
    
    def _learn_adaptation_effectiveness(self, cache_entry: CacheEntry, 
                                      adapted_message: IntrospectiveMessage, 
                                      context: Optional[Dict[str, Any]]):
        """
        Apprend de l'efficacit√© de l'adaptation (version simplifi√©e)
        
        Args:
            cache_entry: Entr√©e du cache
            adapted_message: Message adapt√©
            context: Contexte de l'adaptation
        """
        # Calcul de l'efficacit√© de l'adaptation
        adaptation_effectiveness = self._calculate_effectiveness_score(adapted_message, context)
        
        # Mise √† jour du score d'efficacit√©
        cache_entry.effectiveness_score = (cache_entry.effectiveness_score + adaptation_effectiveness) / 2
    
    async def _learn_successful_adaptation_pattern(self, cache_entry: CacheEntry, context: Optional[Dict[str, Any]]):
        """
        Apprend des patterns d'adaptation r√©ussis
        
        Args:
            cache_entry: Entr√©e du cache
            context: Contexte de l'adaptation r√©ussie
        """
        # Analyse LLM du pattern de succ√®s
        prompt = f"""
Analyse ce pattern d'adaptation r√©ussie pour en extraire les √©l√©ments cl√©s.

Entr√©e du cache:
- Hash: {cache_entry.response_hash[:8]}...
- Score d'efficacit√©: {cache_entry.effectiveness_score:.2f}
- Usage count: {cache_entry.usage_count}

Contexte de l'adaptation r√©ussie:
{json.dumps(context, ensure_ascii=False, indent=2) if context else "Aucun contexte"}

Historique des adaptations:
{json.dumps(cache_entry.adaptation_history, ensure_ascii=False, indent=2)}

Identifie les √©l√©ments cl√©s qui ont contribu√© au succ√®s de cette adaptation.
Retourne un JSON avec les insights:
{{
    "success_factors": ["facteur1", "facteur2"],
    "adaptation_quality": "description de la qualit√©",
    "reusability_score": 0.85
}}
"""
        
        try:
            response = await self.provider.generate_response(prompt, temperature=0.1)
            insights = json.loads(response.content)
            
            # Stockage des insights pour utilisation future
            pattern_key = f"{cache_entry.entity_id}_{cache_entry.entity_type}"
            if pattern_key not in self.learning_patterns:
                self.learning_patterns[pattern_key] = []
            
            self.learning_patterns[pattern_key].append({
                "insights": insights,
                "timestamp": time.time(),
                "effectiveness": cache_entry.effectiveness_score
            })
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur apprentissage pattern: {e}")
    
    def _generate_response_hash(self, response: str) -> str:
        """G√©n√®re un hash de la r√©ponse"""
        return hashlib.sha256(response.encode('utf-8')).hexdigest()
    
    def _create_context_signature(self, entity_id: str, entity_type: str, 
                                context: Optional[Dict[str, Any]]) -> str:
        """Cr√©e une signature contextuelle"""
        signature_data = {
            "entity_id": entity_id,
            "entity_type": entity_type,
            "context": context
        }
        return hashlib.sha256(json.dumps(signature_data, sort_keys=True).encode('utf-8')).hexdigest()
    
    def _evict_least_effective_entry(self):
        """√âvince l'entr√©e la moins efficace du cache"""
        if not self.cache:
            return
        
        # Trouve l'entr√©e avec le score d'efficacit√© le plus bas
        least_effective = min(self.cache.values(), key=lambda x: x.effectiveness_score)
        
        # √âvince l'entr√©e
        del self.cache[least_effective.response_hash]
        print(f"üóëÔ∏è √âvinc√© du cache: {least_effective.response_hash[:8]}... (effectiveness: {least_effective.effectiveness_score:.2f})")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du cache"""
        if not self.cache:
            return {"error": "Cache vide"}
        
        total_entries = len(self.cache)
        avg_effectiveness = sum(entry.effectiveness_score for entry in self.cache.values()) / total_entries
        total_usage = sum(entry.usage_count for entry in self.cache.values())
        
        return {
            "total_entries": total_entries,
            "average_effectiveness": avg_effectiveness,
            "total_usage": total_usage,
            "learning_patterns_count": len(self.learning_patterns),
            "cache_hit_rate": "N/A"  # √Ä impl√©menter avec tracking des hits/misses
        } 