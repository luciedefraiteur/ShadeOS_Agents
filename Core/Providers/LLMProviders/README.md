# ğŸ¤– Core/LLMProviders - SystÃ¨me de Providers LLM

**Date :** 2025-08-07  
**Auteur :** Alma (via Lucie Defraiteur)  
**Contexte :** SystÃ¨me abstrait de providers LLM pour ShadeOS_Agents

---

## ğŸ¯ Vue d'Ensemble

Le module `Core/LLMProviders` fournit un systÃ¨me abstrait et rÃ©utilisable de providers LLM pour tous les composants de ShadeOS_Agents. Il inclut validation, gestion d'erreurs, estimation de taille et configuration flexible.

---

## ğŸ—ï¸ Architecture

### **âœ… Composants Principaux :**

#### **1. Provider Abstrait**
```python
from Core.LLMProviders import (
    # Interface principale
    LLMProvider,
    ProviderStatus,
    LLMResponse,
    ValidationResult,
    ProviderType,
    ErrorType
)
```

#### **2. Providers Concrets**
```python
from Core.LLMProviders import (
    # Providers disponibles
    OpenAIProvider,
    LocalProvider,
    LocalProviderHTTP,
    
    # Factory pattern
    ProviderFactory
)
```

#### **3. Factory Pattern**
```python
from Core.LLMProviders import ProviderFactory

# CrÃ©ation de provider
provider = ProviderFactory.create_provider("openai", api_key="...")

# Validation de provider
validation = await ProviderFactory.validate_provider(provider)
```

---

## ğŸ“ Structure

### **âœ… Core/LLMProviders/**
```
Core/LLMProviders/
â”œâ”€â”€ __init__.py              # Interface principale
â”œâ”€â”€ llm_provider.py          # Classe abstraite LLMProvider
â”œâ”€â”€ provider_factory.py      # Factory pattern
â”œâ”€â”€ openai_provider.py       # Provider OpenAI
â”œâ”€â”€ local_provider.py        # Provider Ollama (subprocess)
â”œâ”€â”€ local_provider_http.py   # Provider Ollama (HTTP)
â””â”€â”€ README.md               # Documentation
```

---

## ğŸ”§ FonctionnalitÃ©s

### **âœ… Providers SupportÃ©s :**

#### **1. OpenAI Provider**
- **ModÃ¨les** : GPT-4, GPT-3.5-turbo, etc.
- **CapacitÃ©s** : Chat completion, text generation, streaming, function calling
- **Configuration** : API key, model, organization, timeout, max_tokens
- **Avantages** : Haute qualitÃ©, fonction calling, streaming

#### **2. Local Provider (HTTP)**
- **ModÃ¨les** : Ollama models (qwen2.5:7b, llama3.1:8b, etc.)
- **CapacitÃ©s** : Text generation, chat completion, local inference
- **Configuration** : Model, ollama_host, timeout, temperature
- **Avantages** : Local, privÃ©, rapide, pas de coÃ»t

#### **3. Local Provider (Subprocess)**
- **ModÃ¨les** : Ollama models via subprocess
- **CapacitÃ©s** : Text generation, chat completion, local inference
- **Configuration** : Model, ollama_binary, timeout, temperature
- **Avantages** : CompatibilitÃ© legacy, contrÃ´le direct

### **âœ… FonctionnalitÃ©s AvancÃ©es :**

#### **1. Validation Automatique**
```python
# Validation complÃ¨te d'un provider
validation = await ProviderFactory.validate_provider(provider)

if validation.valid:
    print(f"Provider {validation.provider_type} validÃ© en {validation.test_time:.2f}s")
else:
    print(f"Erreur: {validation.error}")
```

#### **2. Estimation de Taille**
```python
# Estimation des tokens
estimated_tokens = provider.estimate_tokens(prompt)

# Validation de la taille
is_valid = provider.validate_request_size(prompt, max_tokens=4000)
```

#### **3. Gestion d'Erreurs**
```python
# Types d'erreurs supportÃ©s
from Core.LLMProviders import ErrorType

error_types = [
    ErrorType.CONNECTION_ERROR,
    ErrorType.AUTHENTICATION_ERROR,
    ErrorType.RATE_LIMIT_ERROR,
    ErrorType.QUOTA_EXCEEDED,
    ErrorType.INVALID_REQUEST,
    ErrorType.UNKNOWN_ERROR
]
```

---

## ğŸš€ Utilisation

### **1. CrÃ©ation de Provider :**
```python
from Core.LLMProviders import ProviderFactory

# Provider OpenAI
openai_provider = ProviderFactory.create_provider(
    "openai",
    api_key="sk-...",
    model="gpt-4",
    timeout=30
)

# Provider Local
local_provider = ProviderFactory.create_provider(
    "local",
    model="qwen2.5:7b",
    ollama_host="http://localhost:11434",
    temperature=0.7
)
```

### **2. Validation de Provider :**
```python
# Validation complÃ¨te
provider, validation = await ProviderFactory.create_and_validate_provider(
    "openai",
    api_key="sk-..."
)

if validation.valid:
    print(f"Provider {validation.provider_type} prÃªt")
    print(f"CapacitÃ©s: {validation.capabilities}")
else:
    print(f"Erreur de validation: {validation.error}")
```

### **3. Utilisation du Provider :**
```python
# Appel simple
response = await provider.generate_text(
    prompt="Explique-moi l'intelligence artificielle",
    max_tokens=500
)

# Appel avec streaming
async for chunk in provider.stream_generate_text(
    prompt="Ã‰cris une histoire",
    max_tokens=1000
):
    print(chunk, end="", flush=True)
```

### **4. Gestion d'Erreurs :**
```python
try:
    response = await provider.generate_text(prompt)
except Exception as e:
    if provider.last_error_type == ErrorType.RATE_LIMIT_ERROR:
        # Attendre et rÃ©essayer
        await asyncio.sleep(60)
        response = await provider.generate_text(prompt)
    elif provider.last_error_type == ErrorType.QUOTA_EXCEEDED:
        # Basculer vers provider local
        local_provider = ProviderFactory.create_provider("local")
        response = await local_provider.generate_text(prompt)
```

---

## ğŸ“Š MÃ©triques

### **âœ… Performance :**
- **OpenAI** : Latence 1-3s, haute qualitÃ©
- **Local HTTP** : Latence 0.5-2s, qualitÃ© variable
- **Local Subprocess** : Latence 0.3-1.5s, contrÃ´le direct

### **âœ… FiabilitÃ© :**
- **Validation** : 100% des providers validÃ©s avant utilisation
- **Fallback** : Basculement automatique en cas d'erreur
- **Retry** : Logique de retry intelligente
- **Monitoring** : MÃ©triques de performance en temps rÃ©el

### **âœ… CoÃ»t :**
- **OpenAI** : Pay-per-token (0.01-0.03$/1K tokens)
- **Local** : Gratuit, coÃ»t hardware uniquement
- **Hybride** : Optimisation automatique des coÃ»ts

---

## ğŸ”„ IntÃ©gration

### **âœ… Avec TemporalFractalMemoryEngine :**
```python
from Core.LLMProviders import ProviderFactory
from TemporalFractalMemoryEngine import TemporalEngine

# Provider avec mÃ©moire temporelle
temporal_engine = TemporalEngine()
provider = ProviderFactory.create_provider("openai")

# Enregistrement des appels dans la mÃ©moire
temporal_node = await temporal_engine.create_temporal_node(
    content=f"LLM Call: {prompt[:100]}...",
    metadata={
        "provider": provider.provider_type,
        "model": provider.model,
        "tokens_used": response.usage.total_tokens
    }
)
```

### **âœ… Avec Core/Agents :**
```python
from Core.Agents.V10 import V10Assistant
from Core.LLMProviders import ProviderFactory

# Assistant avec provider configurÃ©
provider = ProviderFactory.create_provider("openai")
assistant = V10Assistant(llm_provider=provider)

# ExÃ©cution avec provider optimisÃ©
result = await assistant.execute_task("Analyse ce code")
```

---

## ğŸ“ DÃ©veloppement

### **âœ… Ajout d'un Nouveau Provider :**
1. **CrÃ©er le provider** : `nouveau_provider.py`
2. **ImplÃ©menter l'interface** : HÃ©riter de `LLMProvider`
3. **Ajouter Ã  la factory** : Dans `provider_factory.py`
4. **Ajouter les tests** : `tests/test_nouveau_provider.py`
5. **Documenter** : Dans le README

### **âœ… Standards de Code :**
- **Type hints** : Obligatoires
- **Docstrings** : Documentation complÃ¨te
- **Tests** : Couverture > 90%
- **Validation** : Validation des entrÃ©es
- **Error handling** : Gestion robuste d'erreurs

---

## ğŸ”— Liens

### **ğŸ“‹ Documentation :**
- [Provider Interface](./llm_provider.py)
- [Factory Pattern](./provider_factory.py)
- [Testing Guide](./tests/README.md)

### **ğŸ“‹ Code :**
- [OpenAI Provider](./openai_provider.py)
- [Local Provider](./local_provider.py)
- [Local HTTP Provider](./local_provider_http.py)

---

**Rapport gÃ©nÃ©rÃ© automatiquement par Alma**  
**Date :** 2025-08-07  
**Statut :** Documentation complÃ¨te du systÃ¨me de providers LLM
