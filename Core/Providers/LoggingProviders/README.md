# üìù Core/LoggingProviders - Syst√®me de Logging Flexible

**Date :** 2025-08-07  
**Auteur :** Alma (via Lucie Defraiteur)  
**Contexte :** Syst√®me de logging modulaire et extensible pour ShadeOS_Agents

---

## üéØ Vue d'Ensemble

Le module `Core/LoggingProviders` fournit un syst√®me de logging flexible et extensible avec des providers sp√©cialis√©s pour diff√©rents contextes. Il supporte le logging console, fichier, et sp√©cialis√© avec des m√©tadonn√©es structur√©es.

---

## üèóÔ∏è Architecture

### **‚úÖ Composants Principaux :**

#### **1. Provider de Base**
```python
from Core.LoggingProviders import (
    # Interface commune
    BaseLoggingProvider,
    
    # Providers sp√©cialis√©s
    ConsoleLoggingProvider,
    FileLoggingProvider,
    ImportAnalyzerLoggingProvider
)
```

#### **2. Providers Disponibles**
- **BaseLoggingProvider** : Interface abstraite commune
- **ConsoleLoggingProvider** : Logging vers console avec couleurs
- **FileLoggingProvider** : Logging vers fichiers avec rotation
- **ImportAnalyzerLoggingProvider** : Logging sp√©cialis√© pour l'analyse d'imports

---

## üìÅ Structure

### **‚úÖ Core/LoggingProviders/**
```
Core/LoggingProviders/
‚îú‚îÄ‚îÄ __init__.py                          # Interface principale
‚îú‚îÄ‚îÄ base_logging_provider.py             # Classe abstraite de base
‚îú‚îÄ‚îÄ console_logging_provider.py          # Provider console
‚îú‚îÄ‚îÄ file_logging_provider.py             # Provider fichier
‚îú‚îÄ‚îÄ import_analyzer_logging_provider.py  # Provider sp√©cialis√©
‚îî‚îÄ‚îÄ README.md                           # Documentation
```

---

## üîß Fonctionnalit√©s

### **‚úÖ BaseLoggingProvider :**

#### **1. Interface Commune**
```python
class BaseLoggingProvider(ABC):
    """Provider de base pour tous les syst√®mes de logging."""
    
    def log_info(self, message: str, **metadata) -> None
    def log_warning(self, message: str, **metadata) -> None
    def log_error(self, message: str, **metadata) -> None
    def log_debug(self, message: str, **metadata) -> None
    def log_structured(self, data: Dict[str, Any], **metadata) -> None
    def log_statistics(self, stats: Dict[str, Any], **metadata) -> None
    def log_performance(self, operation: str, duration: float, **metadata) -> None
```

#### **2. M√©tadonn√©es Structur√©es**
```python
# M√©tadonn√©es automatiques
metadata = {
    'timestamp': '2025-08-07 11:30:45',
    'provider': 'ConsoleLoggingProvider',
    'log_level': 'INFO',
    'custom_field': 'value'
}
```

### **‚úÖ ConsoleLoggingProvider :**

#### **1. Logging Color√©**
```python
from Core.LoggingProviders import ConsoleLoggingProvider

# Provider console avec couleurs
console_provider = ConsoleLoggingProvider(
    log_level="DEBUG",
    use_colors=True,
    timestamp_format="%H:%M:%S"
)

# Logging avec couleurs
console_provider.log_info("Op√©ration r√©ussie", operation="file_analysis")
console_provider.log_warning("Attention", file="main.py")
console_provider.log_error("Erreur critique", error_code=500)
```

#### **2. Formatage Avanc√©**
```python
# Configuration personnalis√©e
console_provider = ConsoleLoggingProvider(
    log_level="INFO",
    use_colors=True,
    show_timestamp=True,
    show_level=True,
    show_provider_name=True
)
```

### **‚úÖ FileLoggingProvider :**

#### **1. Logging vers Fichiers**
```python
from Core.LoggingProviders import FileLoggingProvider

# Provider fichier avec rotation
file_provider = FileLoggingProvider(
    log_directory="logs/",
    log_format="json",
    max_file_size="10MB",
    backup_count=5
)

# Logging structur√©
file_provider.log_structured({
    "operation": "import_analysis",
    "files_processed": 150,
    "imports_resolved": 1200
})
```

#### **2. Rotation Automatique**
```python
# Configuration de rotation
file_provider = FileLoggingProvider(
    log_directory="logs/",
    max_file_size="50MB",      # Taille max par fichier
    backup_count=10,           # Nombre de fichiers de backup
    log_format="json",         # Format JSON structur√©
    compress_old_logs=True     # Compression des anciens logs
)
```

### **‚úÖ ImportAnalyzerLoggingProvider :**

#### **1. Logging Sp√©cialis√©**
```python
from Core.LoggingProviders import ImportAnalyzerLoggingProvider

# Provider sp√©cialis√© pour l'analyse d'imports
import_provider = ImportAnalyzerLoggingProvider(
    analysis_session_id="session_123",
    log_resolution_details=True,
    log_performance_metrics=True,
    log_directory="logs/import_analysis/"
)
```

#### **2. M√©thodes Sp√©cialis√©es**
```python
# Logging du d√©but d'analyse
import_provider.log_analysis_start(
    start_files=["main.py", "utils.py"],
    analysis_depth=3
)

# Logging de r√©solution d'import
import_provider.log_import_resolution(
    import_name="numpy",
    current_file="main.py",
    resolved_path="/usr/lib/python3.9/site-packages/numpy",
    resolution_time=0.002
)

# Logging de r√©sum√©
import_provider.log_imports_summary(
    file_path="main.py",
    local_imports=["utils", "config"],
    standard_imports=["os", "sys"],
    third_party_imports=["numpy", "pandas"]
)
```

---

## üöÄ Utilisation

### **1. Configuration de Base :**
```python
from Core.LoggingProviders import ConsoleLoggingProvider, FileLoggingProvider

# Provider console pour d√©veloppement
dev_logger = ConsoleLoggingProvider(
    log_level="DEBUG",
    use_colors=True
)

# Provider fichier pour production
prod_logger = FileLoggingProvider(
    log_directory="logs/production/",
    log_format="json",
    max_file_size="100MB"
)
```

### **2. Logging Structur√© :**
```python
# Logging de donn√©es structur√©es
logger.log_structured({
    "operation": "code_analysis",
    "files_analyzed": 150,
    "imports_found": 1200,
    "resolution_rate": 0.95
}, session_id="analysis_123")

# Logging de statistiques
logger.log_statistics({
    "total_time": 45.2,
    "average_resolution_time": 0.003,
    "memory_usage": "256MB"
})
```

### **3. Logging de Performance :**
```python
import time

# Mesure de performance
start_time = time.time()
# ... op√©ration ...
duration = time.time() - start_time

logger.log_performance(
    operation="import_resolution",
    duration=duration,
    files_processed=50
)
```

### **4. Logging d'Erreurs :**
```python
try:
    # ... op√©ration risqu√©e ...
    result = risky_operation()
except Exception as e:
    logger.log_error(
        "√âchec de l'op√©ration",
        error_type=type(e).__name__,
        error_message=str(e),
        operation="import_analysis",
        file_path="main.py"
    )
```

---

## üìä M√©triques

### **‚úÖ Performance :**
- **Console** : < 1ms par log
- **Fichier** : < 5ms par log
- **Structured** : < 10ms par log
- **Rotation** : < 100ms pour fichiers < 10MB

### **‚úÖ Fiabilit√© :**
- **Validation** : 100% des entr√©es valid√©es
- **Rotation** : Rotation automatique des fichiers
- **Compression** : Compression des anciens logs
- **R√©cup√©ration** : R√©cup√©ration automatique en cas d'erreur

### **‚úÖ Flexibilit√© :**
- **Formats** : JSON, texte, personnalis√©
- **Niveaux** : DEBUG, INFO, WARNING, ERROR
- **Destinations** : Console, fichier, r√©seau
- **M√©tadonn√©es** : Extensibles et structur√©es

---

## üîÑ Int√©gration

### **‚úÖ Avec TemporalFractalMemoryEngine :**
```python
from Core.LoggingProviders import ImportAnalyzerLoggingProvider
from TemporalFractalMemoryEngine import TemporalEngine

# Provider avec m√©moire temporelle
temporal_engine = TemporalEngine()
logger = ImportAnalyzerLoggingProvider(
    analysis_session_id="temporal_session_123"
)

# Enregistrement dans la m√©moire temporelle
temporal_node = await temporal_engine.create_temporal_node(
    content="Import Analysis Session",
    metadata={
        "session_id": logger.analysis_session_id,
        "stats": logger.get_analysis_report()
    }
)
```

### **‚úÖ Avec Core/Partitioner :**
```python
from Core.LoggingProviders import FileLoggingProvider
from Core.Partitioner import partition_file

# Provider pour le partitionnement
logger = FileLoggingProvider(
    log_directory="logs/partitioning/",
    log_format="json"
)

# Logging des op√©rations de partitionnement
try:
    result = partition_file("main.py")
    logger.log_info(
        "Partitionnement r√©ussi",
        file="main.py",
        blocks_count=len(result.blocks),
        partition_time=result.partition_time
    )
except Exception as e:
    logger.log_error(
        "√âchec du partitionnement",
        file="main.py",
        error=str(e)
    )
```

---

## üìù D√©veloppement

### **‚úÖ Ajout d'un Nouveau Provider :**
1. **Cr√©er le provider** : `nouveau_provider.py`
2. **H√©riter de BaseLoggingProvider** : Impl√©menter les m√©thodes abstraites
3. **Ajouter √† __init__.py** : Export du nouveau provider
4. **Ajouter les tests** : `tests/test_nouveau_provider.py`
5. **Documenter** : Dans le README

### **‚úÖ Standards de Code :**
- **Type hints** : Obligatoires
- **Docstrings** : Documentation compl√®te
- **Tests** : Couverture > 90%
- **Validation** : Validation des entr√©es
- **Error handling** : Gestion robuste d'erreurs

---

## üîó Liens

### **üìã Documentation :**
- [Base Provider](./base_logging_provider.py)
- [Console Provider](./console_logging_provider.py)
- [File Provider](./file_logging_provider.py)
- [Import Analyzer Provider](./import_analyzer_logging_provider.py)

### **üìã Code :**
- [Interface Principale](./__init__.py)
- [Tests](./tests/)

---

**Rapport g√©n√©r√© automatiquement par Alma**  
**Date :** 2025-08-07  
**Statut :** Documentation compl√®te du syst√®me de logging
