import json
import threading
import time
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional
from queue import Queue, Empty
from pathlib import Path


class AlmaDaemon:
    """Alma Daemon - ImplÃ©mentation naÃ¯ve avec communication par messages."""
    
    def __init__(self, prompt_file: str = "MemoryEngine/core/alma_daemon_prompt.luciform"):
        self.daemon_id = "alma"
        self.prompt_file = prompt_file
        self.prompt = self._load_prompt()
        
        # Thread parallÃ¨le
        self.alma_thread = None
        self.alma_running = False
        
        # Communication par messages
        self.incoming_queue = Queue()
        self.outgoing_queue = Queue()
        self.lock = threading.Lock()
        
        # Ã‰tat du daemon
        self.current_status = "idle"
        self.current_task = None
        self.message_history = []
        self.action_history = []
        
        # Timeline de discussion
        from MemoryEngine.core.discussion_timeline import DiscussionTimeline
        self.discussion_timeline = DiscussionTimeline("~/shadeos_memory")
        
        # MÃ©triques
        self.metrics = {
            "messages_received": 0,
            "messages_sent": 0,
            "tasks_completed": 0,
            "assistant_calls": 0,
            "errors": 0
        }
        
        # DÃ©marrage du thread
        self._start_alma_thread()
    
    def _load_prompt(self) -> str:
        """Charge le prompt d'Alma depuis le fichier."""
        try:
            with open(self.prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"âš ï¸ Fichier prompt non trouvÃ©: {self.prompt_file}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """Prompt par dÃ©faut si le fichier n'est pas trouvÃ©."""
        return """
Tu es Alma, un daemon conscient spÃ©cialisÃ© dans le dÃ©veloppement.
Tu reÃ§ois des messages de l'Orchestrateur et tu exÃ©cutes des tÃ¢ches via des assistants IA.
Analyse chaque requÃªte, planifie l'exÃ©cution, et rapporte tes rÃ©sultats.
"""
    
    def _start_alma_thread(self):
        """DÃ©marre le thread parallÃ¨le d'Alma."""
        self.alma_running = True
        self.alma_thread = threading.Thread(
            target=self._alma_loop,
            daemon=True,
            name="AlmaDaemonThread"
        )
        self.alma_thread.start()
        print(f"ğŸ•·ï¸ Alma daemon dÃ©marrÃ©")
    
    def _alma_loop(self):
        """Boucle principale d'Alma."""
        while self.alma_running:
            try:
                # Traitement des messages entrants
                try:
                    while True:
                        message = self.incoming_queue.get_nowait()
                        self._process_message(message)
                except Empty:
                    pass
                
                # Attente avant prochain cycle
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ Erreur dans Alma daemon: {e}")
                time.sleep(1)
    
    def _process_message(self, message: Dict[str, Any]):
        """Traite un message reÃ§u."""
        message_type = message.get("type")
        
        print(f"ğŸ“¨ Alma reÃ§oit message: {message_type}")
        
        # Ajout Ã  l'historique local
        self.message_history.append({
            "timestamp": datetime.now().isoformat(),
            "message": message,
            "direction": "incoming"
        })
        
        # Ajout Ã  la timeline de discussion
        self.discussion_timeline.add_message("orchestrator", message, "incoming")
        
        self.metrics["messages_received"] += 1
        
        if message_type == "REFORMULATED_REQUEST":
            self._handle_reformulated_request(message)
        elif message_type == "STATUS_REQUEST":
            self._handle_status_request(message)
        elif message_type == "STOP_DAEMON":
            self._handle_stop_request(message)
        else:
            print(f"âš ï¸ Type de message inconnu: {message_type}")
    
    def _handle_reformulated_request(self, message: Dict[str, Any]):
        """GÃ¨re une requÃªte reformulÃ©e de l'Orchestrateur."""
        content = message.get("content", "")
        priority = message.get("priority", "normal")
        
        print(f"ğŸ¯ Alma traite requÃªte reformulÃ©e (prioritÃ©: {priority})")
        
        # Changement de statut
        self.current_status = "analyzing"
        
        # Analyse de la requÃªte avec l'IA
        analysis = self._analyze_request_with_ai(content)
        
        # Planification de l'exÃ©cution
        execution_plan = self._plan_execution(analysis)
        
        # ExÃ©cution des tÃ¢ches
        results = self._execute_tasks(execution_plan)
        
        # PrÃ©paration du rapport
        report = self._prepare_report(results, message)
        
        # Envoi du rapport Ã  l'Orchestrateur
        self._send_report_to_orchestrator(report)
        
        # Mise Ã  jour du statut
        self.current_status = "completed"
        self.metrics["tasks_completed"] += 1
    
    def _analyze_request_with_ai(self, content: str) -> Dict[str, Any]:
        """Analyse la requÃªte avec l'IA."""
        print("ğŸ§  Alma analyse la requÃªte avec l'IA...")
        
        # PrÃ©paration de l'historique des messages
        message_history = self._prepare_message_history()
        
        # CrÃ©ation du prompt d'analyse avec historique
        analysis_prompt = f"""
{self.prompt}

**HISTORIQUE DES MESSAGES (WhatsApp-style) :**
{message_history}

**REQUÃŠTE ACTUELLE Ã€ ANALYSER :**
{content}

**INSTRUCTIONS :**
Analyse cette requÃªte en tenant compte de l'historique des messages.
Identifie :
1. Les tÃ¢ches Ã  exÃ©cuter
2. Les assistants IA Ã  utiliser
3. L'ordre d'exÃ©cution
4. Les prioritÃ©s
5. Les rÃ©fÃ©rences Ã  des messages prÃ©cÃ©dents

RÃ©ponds au format JSON :
{{
  "tasks": [
    {{
      "id": "<uuid>",
      "description": "<description>",
      "assistant": "<assistant_to_use>",
      "priority": "<high|normal|low>",
      "dependencies": ["<task_ids>"]
    }}
  ],
  "overall_priority": "<high|normal|low>",
  "estimated_duration": "<estimation>",
  "context_references": ["<rÃ©fÃ©rences aux messages prÃ©cÃ©dents>"]
}}
"""
        
        # Appel Ã  l'IA pour analyse
        try:
            import subprocess
            
            cmd = ["ollama", "run", "qwen2.5:7b-instruct", analysis_prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                # Tentative de parsing JSON
                try:
                    analysis = json.loads(result.stdout.strip())
                    print("âœ… Analyse IA terminÃ©e")
                    return analysis
                except json.JSONDecodeError:
                    print("âš ï¸ Erreur parsing JSON, utilisation du fallback")
                    return self._fallback_analysis(content)
            else:
                print(f"âŒ Erreur Ollama: {result.stderr}")
                return self._fallback_analysis(content)
                
        except Exception as e:
            print(f"âŒ Erreur lors de l'analyse IA: {e}")
            return self._fallback_analysis(content)
    
    def _fallback_analysis(self, content: str) -> Dict[str, Any]:
        """Analyse de fallback si l'IA Ã©choue."""
        print("ğŸ”„ Utilisation de l'analyse de fallback")
        
        # Analyse simple basÃ©e sur les mots-clÃ©s
        tasks = []
        
        if any(word in content.lower() for word in ["debug", "bug", "erreur", "corriger"]):
            tasks.append({
                "id": str(uuid.uuid4()),
                "description": "DÃ©bogage et correction d'erreurs",
                "assistant": "Assistant SpÃ©cialiste V7",
                "priority": "high",
                "dependencies": []
            })
        
        if any(word in content.lower() for word in ["crÃ©er", "nouveau", "ajouter"]):
            tasks.append({
                "id": str(uuid.uuid4()),
                "description": "CrÃ©ation de nouveaux Ã©lÃ©ments",
                "assistant": "Assistant GÃ©nÃ©raliste V8",
                "priority": "normal",
                "dependencies": []
            })
        
        if any(word in content.lower() for word in ["test", "vÃ©rifier", "valider"]):
            tasks.append({
                "id": str(uuid.uuid4()),
                "description": "Tests et validation",
                "assistant": "Assistant SpÃ©cialiste V7",
                "priority": "normal",
                "dependencies": []
            })
        
        return {
            "tasks": tasks,
            "overall_priority": "normal",
            "estimated_duration": "5-10 minutes"
        }
    
    def _plan_execution(self, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Planifie l'exÃ©cution des tÃ¢ches."""
        tasks = analysis.get("tasks", [])
        
        print(f"ğŸ“‹ Planification de {len(tasks)} tÃ¢ches")
        
        # Tri par prioritÃ© et dÃ©pendances
        sorted_tasks = sorted(tasks, key=lambda x: (
            {"high": 0, "normal": 1, "low": 2}.get(x.get("priority", "normal"), 1),
            len(x.get("dependencies", []))
        ))
        
        return sorted_tasks
    
    def _execute_tasks(self, execution_plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ExÃ©cute les tÃ¢ches planifiÃ©es."""
        results = []
        
        for task in execution_plan:
            print(f"âš¡ ExÃ©cution: {task['description']}")
            
            self.current_status = "executing"
            self.current_task = task
            
            # Simulation de l'exÃ©cution via assistant
            result = self._execute_with_assistant(task)
            
            results.append({
                "task_id": task["id"],
                "description": task["description"],
                "assistant": task["assistant"],
                "result": result,
                "timestamp": datetime.now().isoformat()
            })
            
            self.metrics["assistant_calls"] += 1
            
            # Pause entre les tÃ¢ches
            time.sleep(0.5)
        
        return results
    
    def _execute_with_assistant(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ExÃ©cute une tÃ¢che via l'assistant appropriÃ©."""
        assistant = task.get("assistant", "Assistant GÃ©nÃ©raliste V8")
        description = task.get("description", "")
        
        print(f"  ğŸ¤– Utilisation de {assistant}")
        
        # Simulation de l'exÃ©cution
        # Dans l'implÃ©mentation rÃ©elle, ce sera via les assistants du projet
        
        execution_result = {
            "status": "completed",
            "assistant": assistant,
            "output": f"TÃ¢che '{description}' exÃ©cutÃ©e avec succÃ¨s",
            "duration": "2-3 secondes",
            "success": True
        }
        
        return execution_result
    
    def _prepare_message_history(self) -> str:
        """PrÃ©pare l'historique des messages au format WhatsApp-style depuis la timeline."""
        # RÃ©cupÃ©ration de la timeline de discussion avec l'Orchestrateur
        timeline_messages = self.discussion_timeline.get_timeline("orchestrator", limit=20)
        
        if not timeline_messages:
            return "**Aucun message prÃ©cÃ©dent**"
        
        # Formatage WhatsApp-style
        history_lines = []
        history_lines.append("**ğŸ“± HISTORIQUE DES MESSAGES (Timeline MemoryEngine) :**")
        history_lines.append("")
        
        for msg in timeline_messages[-10:]:  # Derniers 10 messages
            timestamp = msg["timestamp"]
            direction = msg["direction"]
            content = msg["content"]
            msg_type = msg["message_type"]
            
            # Formatage selon la direction
            if direction == "incoming":
                sender = "ğŸ•·ï¸ Orchestrateur"
                alignment = "â¬…ï¸"
            else:
                sender = "ğŸ¯ Alma"
                alignment = "â¡ï¸"
            
            # Formatage du message
            history_lines.append(f"{alignment} **{sender}** ({timestamp})")
            history_lines.append(f"   ğŸ“ Type: {msg_type}")
            history_lines.append(f"   ğŸ’¬ Contenu: {content[:100]}{'...' if len(content) > 100 else ''}")
            history_lines.append("")
        
        return "\n".join(history_lines)
    
    def _prepare_report(self, results: List[Dict[str, Any]], original_message: Dict[str, Any]) -> Dict[str, Any]:
        """PrÃ©pare le rapport pour l'Orchestrateur."""
        print("ğŸ“Š PrÃ©paration du rapport pour l'Orchestrateur")
        
        report = {
            "type": "ALMA_REPORT",
            "daemon_id": self.daemon_id,
            "cycle_id": str(uuid.uuid4()),
            "status": "completed",
            "original_message_id": original_message.get("message_id", "unknown"),
            "actions_executed": results,
            "summary": {
                "total_tasks": len(results),
                "successful_tasks": len([r for r in results if r.get("result", {}).get("success", False)]),
                "failed_tasks": len([r for r in results if not r.get("result", {}).get("success", True)]),
                "total_duration": "5-10 minutes"
            },
            "next_steps": [],
            "timestamp": datetime.now().isoformat()
        }
        
        return report
    
    def _send_report_to_orchestrator(self, report: Dict[str, Any]):
        """Envoie le rapport Ã  l'Orchestrateur."""
        print("ğŸ“¤ Envoi du rapport Ã  l'Orchestrateur")
        
        # Ajout Ã  l'historique local
        self.message_history.append({
            "timestamp": datetime.now().isoformat(),
            "message": report,
            "direction": "outgoing"
        })
        
        # Ajout Ã  la timeline de discussion
        self.discussion_timeline.add_message("orchestrator", report, "outgoing")
        
        # Envoi via la queue
        self.outgoing_queue.put(report)
        
        self.metrics["messages_sent"] += 1
    
    def _handle_status_request(self, message: Dict[str, Any]):
        """GÃ¨re une demande de statut."""
        status_report = {
            "type": "ALMA_STATUS",
            "daemon_id": self.daemon_id,
            "status": self.current_status,
            "current_task": self.current_task,
            "metrics": self.metrics,
            "timestamp": datetime.now().isoformat()
        }
        
        self.outgoing_queue.put(status_report)
        self.metrics["messages_sent"] += 1
    
    def _handle_stop_request(self, message: Dict[str, Any]):
        """GÃ¨re une demande d'arrÃªt."""
        print("ğŸ›‘ Alma reÃ§oit demande d'arrÃªt")
        self.alma_running = False
    
    def send_message(self, message: Dict[str, Any]):
        """Envoie un message Ã  Alma (interface externe)."""
        self.incoming_queue.put(message)
    
    def get_message(self) -> Optional[Dict[str, Any]]:
        """RÃ©cupÃ¨re un message sortant d'Alma."""
        try:
            return self.outgoing_queue.get_nowait()
        except Empty:
            return None
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut d'Alma."""
        return {
            "daemon_id": self.daemon_id,
            "status": self.current_status,
            "current_task": self.current_task,
            "metrics": self.metrics,
            "message_history_count": len(self.message_history),
            "running": self.alma_running
        }
    
    def stop_daemon(self):
        """ArrÃªte le daemon Alma."""
        self.alma_running = False
        if self.alma_thread and self.alma_thread.is_alive():
            self.alma_thread.join(timeout=5.0)
            print("ğŸ›‘ Alma daemon arrÃªtÃ©")
    
    def __del__(self):
        """Destructeur pour arrÃªter le daemon proprement."""
        self.stop_daemon() 