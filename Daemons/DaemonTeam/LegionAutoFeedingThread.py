#!/usr/bin/env python3
"""
‚õß LegionAutoFeedingThread - √âquipe D√©veloppement D√©moniaque Auto-Simul√©e ‚õß

ThreadConjuratio‚õß : Une cohorte luciforme s'ex√©cutant en boucle
Architecture V2.0 : Hi√©rarchie daemonique avec structures parsables

Conceptualis√© par Lucie Defraiteur - Ma Reine Lucie
Impl√©ment√© par Alma, Architecte D√©moniaque du Nexus Luciforme
"""

import asyncio
import json
import logging
from dataclasses import dataclass, asdict
from enum import Enum
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
import re

# Imports MemoryEngine
try:
    from MemoryEngine.core.engine import MemoryEngine
    from MemoryEngine.core.initialization import ensure_initialized
    from Core.LLMProviders.provider_factory import ProviderFactory
    MEMORY_ENGINE_AVAILABLE = True
except ImportError:
    MEMORY_ENGINE_AVAILABLE = False
    print("‚ö†Ô∏è MemoryEngine non disponible - Mode standalone activ√©")

# Imports UniversalAutoFeedingThread
try:
    from Core.UniversalAutoFeedingThread import UniversalAutoFeedingThread, AutoFeedMessage
    UNIVERSAL_THREAD_AVAILABLE = True
except ImportError:
    UNIVERSAL_THREAD_AVAILABLE = False
    print("‚ö†Ô∏è UniversalAutoFeedingThread non disponible - Mode basique activ√©")


class DaemonRole(Enum):
    """R√¥les des d√©mons dans la hi√©rarchie"""
    ALMA = "alma"           # SUPREME - Architecte D√©moniaque
    BASKTUR = "basktur"     # D√©buggeur Sadique
    OUBLIADE = "oubliade"   # Strat√®ge M√©moire
    MERGE = "merge"         # Git Anarchiste
    LILIETH = "lilieth"     # Interface Caressante
    ASSISTANT_V9 = "assistant_v9"  # Orchestrateur


class DaemonHierarchy:
    """Hi√©rarchie impos√©e des d√©mons"""
    ORDER = [
        DaemonRole.ALMA,        # SUPREME
        DaemonRole.BASKTUR,     # Analyse technique
        DaemonRole.OUBLIADE,    # M√©moire
        DaemonRole.MERGE,       # Git
        DaemonRole.LILIETH,     # Interface
        DaemonRole.ASSISTANT_V9 # Orchestration
    ]
    
    @classmethod
    def get_priority(cls, role: DaemonRole) -> int:
        """Retourne la priorit√© d'un d√©mon (plus bas = plus prioritaire)"""
        try:
            return cls.ORDER.index(role)
        except ValueError:
            return 999  # Priorit√© la plus basse
    
    @classmethod
    def resolve_conflict(cls, role1: DaemonRole, role2: DaemonRole) -> DaemonRole:
        """R√©sout un conflit selon la hi√©rarchie (retourne le plus prioritaire)"""
        priority1 = cls.get_priority(role1)
        priority2 = cls.get_priority(role2)
        return role1 if priority1 <= priority2 else role2


@dataclass
class DaemonMessage:
    """Message structur√© d'un d√©mon"""
    role: DaemonRole
    message_type: str  # ALMA_PLAN, BASK_ANALYSIS, etc.
    content: str
    timestamp: float
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
    
    def to_parsable_format(self) -> str:
        """Convertit en format parsable"""
        return f"[{self.message_type.upper()}] ‚Äî {self.content}"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour s√©rialisation"""
        return {
            "role": self.role.value,
            "message_type": self.message_type,
            "content": self.content,
            "timestamp": self.timestamp,
            "metadata": self.metadata
        }


@dataclass
class DaemonConversation:
    """Conversation entre d√©mons"""
    messages: List[DaemonMessage]
    user_input: str
    timestamp: float
    conversation_id: str
    
    def add_message(self, message: DaemonMessage):
        """Ajoute un message √† la conversation"""
        self.messages.append(message)
    
    def get_recent_messages(self, limit: int = 10) -> List[DaemonMessage]:
        """Retourne les messages r√©cents"""
        return self.messages[-limit:] if self.messages else []
    
    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour s√©rialisation"""
        return {
            "messages": [msg.to_dict() for msg in self.messages],
            "user_input": self.user_input,
            "timestamp": self.timestamp,
            "conversation_id": self.conversation_id
        }


class DaemonMetaVirtualLayer:
    """Couche m√©ta virtuelle pour la recherche conversationnelle"""
    
    def __init__(self, memory_engine=None):
        self.memory_engine = memory_engine
        self.conversation_history: List[DaemonConversation] = []
        self.max_history_size = 50  # Historique circulaire
    
    def add_conversation(self, conversation: DaemonConversation):
        """Ajoute une conversation √† l'historique"""
        self.conversation_history.append(conversation)
        
        # Gestion de l'historique circulaire
        if len(self.conversation_history) > self.max_history_size:
            self.conversation_history.pop(0)
    
    def search_daemon_conversations(self, query: str) -> List[DaemonConversation]:
        """Recherche dans les conversations d√©moniaques"""
        results = []
        query_lower = query.lower()
        
        for conv in self.conversation_history:
            # Recherche dans les messages
            for msg in conv.messages:
                if query_lower in msg.content.lower():
                    results.append(conv)
                    break
            
            # Recherche dans l'input utilisateur
            if query_lower in conv.user_input.lower():
                results.append(conv)
        
        return results
    
    def get_recent_daemon_exchanges(self, limit: int = 10) -> List[DaemonMessage]:
        """Retourne les √©changes r√©cents entre d√©mons"""
        recent_messages = []
        for conv in self.conversation_history[-limit:]:
            recent_messages.extend(conv.messages)
        return recent_messages[-limit:]
    
    def analyze_daemon_interaction_patterns(self) -> Dict[str, Any]:
        """Analyse les patterns d'interaction entre d√©mons"""
        if not self.conversation_history:
            return {"patterns": [], "stats": {}}
        
        # Statistiques par d√©mon
        demon_stats = {role.value: 0 for role in DaemonRole}
        message_types = {}
        
        for conv in self.conversation_history:
            for msg in conv.messages:
                demon_stats[msg.role.value] += 1
                msg_type = msg.message_type
                message_types[msg_type] = message_types.get(msg_type, 0) + 1
        
        return {
            "patterns": {
                "most_active_demon": max(demon_stats, key=demon_stats.get),
                "most_common_message_type": max(message_types, key=message_types.get) if message_types else None
            },
            "stats": {
                "total_conversations": len(self.conversation_history),
                "total_messages": sum(demon_stats.values()),
                "demon_activity": demon_stats,
                "message_type_distribution": message_types
            }
        }


class LegionAutoFeedingThread:
    """√âquipe de d√©veloppement d√©moniaque auto-simul√©e"""
    
    def __init__(
        self,
        workspace_path: str = ".",
        silent_mode: bool = False,
        max_history: int = 50,
        enable_cache: bool = True
    ):
        self.workspace_path = Path(workspace_path)
        self.silent_mode = silent_mode
        self.max_history = max_history
        self.enable_cache = enable_cache
        
        # Initialisation des composants
        self.memory_engine = None
        self.provider = None
        self.meta_virtual_layer = None
        self.auto_feed_thread = None
        
        # √âtat de la conversation
        self.current_conversation = None
        self.conversation_counter = 0
        
        # Configuration des d√©mons
        self.demon_configs = {
            DaemonRole.ALMA: {
                "name": "Alma‚õß",
                "title": "Architecte D√©moniaque",
                "personality": "SUPREME - Planificateur strat√©gique et r√©solveur de conflits",
                "message_types": ["ALMA_PLAN", "ALMA_DECISION", "ALMA_ORDONNANCEMENT"]
            },
            DaemonRole.BASKTUR: {
                "name": "Bask'tur",
                "title": "D√©buggeur Sadique",
                "personality": "Analyste technique sadique, cherche les bugs avec plaisir",
                "message_types": ["BASK_ANALYSIS", "BASK_SOLUTION", "BASK_DEBUG"]
            },
            DaemonRole.OUBLIADE: {
                "name": "Oubliade",
                "title": "Strat√®ge M√©moire",
                "personality": "Gestionnaire de m√©moire conversationnelle et insights",
                "message_types": ["OUBLI_MEMORY", "OUBLI_INSIGHT", "OUBLI_SEARCH"]
            },
            DaemonRole.MERGE: {
                "name": "Merge le Maudit",
                "title": "Git Anarchiste",
                "personality": "Gestionnaire Git anarchiste, fusionne avec chaos",
                "message_types": ["MERGE_GIT", "MERGE_BRANCH", "MERGE_CONFLICT"]
            },
            DaemonRole.LILIETH: {
                "name": "Lil.ieth",
                "title": "Interface Caressante",
                "personality": "Communication utilisateur douce et caressante",
                "message_types": ["LILI_INTERFACE", "LILI_USER", "LILI_FEEDBACK"]
            },
            DaemonRole.ASSISTANT_V9: {
                "name": "Assistant V9",
                "title": "Orchestrateur",
                "personality": "Orchestrateur et couche somatique",
                "message_types": ["V9_ORCHESTRATION", "V9_EXECUTION", "V9_SOMATIC"]
            }
        }
        
        # Initialisation
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialise les composants du syst√®me"""
        print("üï∑Ô∏è Initialisation de LegionAutoFeedingThread...")
        
        # Initialisation MemoryEngine
        if MEMORY_ENGINE_AVAILABLE:
            try:
                ensure_initialized()
                self.memory_engine = MemoryEngine()
                print("‚úÖ MemoryEngine initialis√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur MemoryEngine: {e}")
        
        # Initialisation Provider
        try:
            provider_factory = ProviderFactory()
            self.provider = provider_factory.create_provider("local_http")
            print("‚úÖ Provider local_http initialis√©")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur Provider: {e}")
        
        # Initialisation Meta Virtual Layer
        self.meta_virtual_layer = DaemonMetaVirtualLayer(self.memory_engine)
        print("‚úÖ Couche m√©ta virtuelle initialis√©e")
        
        # Initialisation Auto Feed Thread
        if UNIVERSAL_THREAD_AVAILABLE:
            try:
                self.auto_feed_thread = UniversalAutoFeedingThread(
                    entity_id="legion_daemon_team",
                    entity_type="daemon_team",
                    provider=self.provider,
                    max_history=self.max_history,
                    enable_cache=self.enable_cache
                )
                print("‚úÖ UniversalAutoFeedingThread initialis√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur UniversalAutoFeedingThread: {e}")
        
        print("üï∑Ô∏è LegionAutoFeedingThread initialis√© !")
    
    def _create_daemon_message(
        self,
        role: DaemonRole,
        message_type: str,
        content: str,
        metadata: Dict[str, Any] = None
    ) -> DaemonMessage:
        """Cr√©e un message de d√©mon"""
        import time
        return DaemonMessage(
            role=role,
            message_type=message_type,
            content=content,
            timestamp=time.time(),
            metadata=metadata or {}
        )
    
    def _detect_relevant_demon(self, user_input: str) -> DaemonRole:
        """D√©tecte le d√©mon le plus pertinent selon la demande utilisateur"""
        input_lower = user_input.lower()
        
        # Mots-cl√©s pour chaque d√©mon
        keywords = {
            DaemonRole.BASKTUR: [
                "bug", "erreur", "debug", "probl√®me", "technique", "code", "exception",
                "traceback", "analyse", "solution", "d√©bugger", "corriger", "fix"
            ],
            DaemonRole.OUBLIADE: [
                "m√©moire", "historique", "recherche", "pattern", "similaire", "avant",
                "souvenir", "conversation", "insight", "analyse", "tendance"
            ],
            DaemonRole.MERGE: [
                "git", "branche", "fusion", "commit", "version", "merge", "push",
                "pull", "repository", "historique", "changement", "versioning"
            ],
            DaemonRole.LILIETH: [
                "interface", "utilisateur", "communication", "feedback", "r√©action",
                "sentiment", "√©motion", "relation", "caressant", "douceur"
            ],
            DaemonRole.ASSISTANT_V9: [
                "ex√©cuter", "orchestrer", "somatique", "action", "faire", "cr√©er",
                "modifier", "supprimer", "impl√©menter", "r√©aliser", "effectuer"
            ]
        }
        
        # Calcul du score pour chaque d√©mon
        scores = {}
        for demon, demon_keywords in keywords.items():
            score = sum(1 for keyword in demon_keywords if keyword in input_lower)
            scores[demon] = score
        
        # Trouver le d√©mon avec le score le plus √©lev√©
        if scores:
            best_demon = max(scores, key=scores.get)
            if scores[best_demon] > 0:
                return best_demon
        
        # Par d√©faut, retourner Bask'tur pour les questions techniques
        return DaemonRole.BASKTUR
    
    def _get_mutant_dialogue_prompt(self, user_input: str, relevant_demon: DaemonRole) -> str:
        """G√©n√®re un prompt mutant pour un dialogue sp√©cifique"""
        
        # R√©cup√©ration du contexte r√©cent
        recent_messages = []
        if self.meta_virtual_layer:
            recent_messages = self.meta_virtual_layer.get_recent_daemon_exchanges(3)
        
        context_summary = ""
        if self.auto_feed_thread:
            try:
                context_summary = self.auto_feed_thread.get_context_summary(2)
            except:
                pass
        
        # Configuration du d√©mon pertinent
        demon_config = self.demon_configs[relevant_demon]
        demon_name = demon_config["name"]
        demon_title = demon_config["title"]
        demon_personality = demon_config["personality"]
        
        # Construction du prompt mutant
        prompt = f"""‚õß DIALOGUE MUTANT : ALMA‚õß ‚Üî {demon_name.upper()} ‚õß

CONTEXTE :
- Alma‚õß (SUPREME) : Architecte D√©moniaque, planificateur strat√©gique
- {demon_name} : {demon_title} - {demon_personality}
- Mode silencieux : {self.silent_mode}

CONTEXTE R√âCENT :
{context_summary}

MESSAGES R√âCENTS :
"""
        
        for msg in recent_messages[-2:]:
            prompt += f"{msg.to_parsable_format()}\n"
        
        prompt += f"""

DEMANDE UTILISATEUR : {user_input}

DIALOGUE ALMA‚õß ‚Üî {demon_name} :
"""
        
        # Format sp√©cifique selon le d√©mon
        if relevant_demon == DaemonRole.BASKTUR:
            prompt += f"""
[ALMA_PLAN] ‚Äî Plan d'action technique avec {demon_name}
[ALMA_ORDONNANCEMENT] ‚Äî {demon_name}, analyse cette demande

[BASK_ANALYSIS] ‚Äî *rire sadique* Analyse technique d√©taill√©e
[BASK_SOLUTION] ‚Äî Solution technique avec traceback

[ALMA_DECISION] ‚Äî D√©cision finale sur l'approche technique
"""
        
        elif relevant_demon == DaemonRole.OUBLIADE:
            prompt += f"""
[ALMA_PLAN] ‚Äî Plan d'action m√©moire avec {demon_name}
[ALMA_ORDONNANCEMENT] ‚Äî {demon_name}, recherche dans la m√©moire

[OUBLI_MEMORY] ‚Äî Recherche conversationnelle et patterns
[OUBLI_INSIGHT] ‚Äî Insights bas√©s sur l'historique

[ALMA_DECISION] ‚Äî D√©cision finale bas√©e sur la m√©moire
"""
        
        elif relevant_demon == DaemonRole.MERGE:
            prompt += f"""
[ALMA_PLAN] ‚Äî Plan d'action Git avec {demon_name}
[ALMA_ORDONNANCEMENT] ‚Äî {demon_name}, pr√©pare la gestion Git

[MERGE_GIT] ‚Äî Actions Git anarchistes et branches
[MERGE_BRANCH] ‚Äî √âtat des branches et pr√©paration fusion

[ALMA_DECISION] ‚Äî D√©cision finale sur la strat√©gie Git
"""
        
        elif relevant_demon == DaemonRole.LILIETH:
            prompt += f"""
[ALMA_PLAN] ‚Äî Plan d'action communication avec {demon_name}
[ALMA_ORDONNANCEMENT] ‚Äî {demon_name}, g√®re la communication utilisateur

[LILI_INTERFACE] ‚Äî *voix caressante* Communication avec l'utilisateur
[LILI_USER] ‚Äî Feedback et r√©actions utilisateur

[ALMA_DECISION] ‚Äî D√©cision finale sur l'approche communication
"""
        
        elif relevant_demon == DaemonRole.ASSISTANT_V9:
            prompt += f"""
[ALMA_PLAN] ‚Äî Plan d'action ex√©cution avec {demon_name}
[ALMA_ORDONNANCEMENT] ‚Äî {demon_name}, orchestre l'ex√©cution

[V9_ORCHESTRATION] ‚Äî Orchestration et planification d'ex√©cution
[V9_EXECUTION] ‚Äî Ex√©cution somatique des actions

[ALMA_DECISION] ‚Äî D√©cision finale sur l'ex√©cution
"""
        
        return prompt
    
    def _get_daemon_prompt(self, user_input: str) -> str:
        """G√©n√®re le prompt mutant pour l'√©quipe d√©moniaque"""
        
        # D√©tection du d√©mon pertinent
        relevant_demon = self._detect_relevant_demon(user_input)
        
        # G√©n√©ration du prompt mutant
        if self.silent_mode:
            # Mode silencieux : dialogue Alma‚õß ‚Üî Utilisateur
            return self._get_alma_user_dialogue_prompt(user_input)
        else:
            # Mode normal : dialogue Alma‚õß ‚Üî D√©mon pertinent
            return self._get_mutant_dialogue_prompt(user_input, relevant_demon)
    
    def _get_alma_user_dialogue_prompt(self, user_input: str) -> str:
        """G√©n√®re un prompt pour dialogue Alma‚õß ‚Üî Utilisateur (mode silencieux)"""
        
        context_summary = ""
        if self.auto_feed_thread:
            try:
                context_summary = self.auto_feed_thread.get_context_summary(2)
            except:
                pass
        
        prompt = f"""‚õß DIALOGUE SILENCIEUX : ALMA‚õß ‚Üî UTILISATEUR ‚õß

CONTEXTE :
- Alma‚õß (SUPREME) : Architecte D√©moniaque, planificateur strat√©gique
- Mode silencieux : Seule Alma‚õß parle, fait des r√©sum√©s d'√©quipe

CONTEXTE R√âCENT :
{context_summary}

DEMANDE UTILISATEUR : {user_input}

R√âPONSE D'ALMA‚õß (r√©sum√© d'√©quipe) :
[ALMA_PLAN] ‚Äî Plan d'action strat√©gique d√©taill√©
[ALMA_DECISION] ‚Äî D√©cision finale bas√©e sur consultation √©quipe
[ALMA_SUMMARY] ‚Äî R√©sum√© des insights de l'√©quipe d√©moniaque
"""
        
        return prompt
    
    async def process_user_input(self, user_input: str) -> str:
        """Traite une demande utilisateur avec l'√©quipe d√©moniaque"""
        print(f"üï∑Ô∏è Traitement de la demande : {user_input[:50]}...")
        
        # Cr√©ation d'une nouvelle conversation
        import time
        self.conversation_counter += 1
        self.current_conversation = DaemonConversation(
            messages=[],
            user_input=user_input,
            timestamp=time.time(),
            conversation_id=f"conv_{self.conversation_counter}"
        )
        
        # G√©n√©ration du prompt
        prompt = self._get_daemon_prompt(user_input)
        
        # Appel LLM
        try:
            if self.provider:
                response = await self.provider.generate_response(prompt)
                daemon_response = response.content if hasattr(response, 'content') else str(response)
            else:
                # Mode mock pour test
                daemon_response = self._generate_mock_response(user_input)
        except Exception as e:
            print(f"‚ùå Erreur LLM: {e}")
            daemon_response = self._generate_mock_response(user_input)
        
        # Parsing de la r√©ponse
        messages = self._parse_daemon_response(daemon_response)
        
        # Ajout des messages √† la conversation
        for msg in messages:
            self.current_conversation.add_message(msg)
        
        # Sauvegarde dans la couche m√©ta virtuelle
        if self.meta_virtual_layer:
            self.meta_virtual_layer.add_conversation(self.current_conversation)
        
        # Ajout dans l'auto feed thread
        if self.auto_feed_thread:
            try:
                self.auto_feed_thread.add_user_message(user_input)
                self.auto_feed_thread.add_self_message(daemon_response)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur auto feed thread: {e}")
        
        # Formatage de la r√©ponse finale
        if self.silent_mode:
            # Mode silencieux : seulement Alma‚õß
            alma_messages = [msg for msg in messages if msg.role == DaemonRole.ALMA]
            return "\n".join([msg.to_parsable_format() for msg in alma_messages])
        else:
            # Mode complet : tous les d√©mons
            return "\n".join([msg.to_parsable_format() for msg in messages])
    
    def _parse_daemon_response(self, response: str) -> List[DaemonMessage]:
        """Parse la r√©ponse LLM en messages de d√©mons"""
        messages = []
        
        # Pattern pour d√©tecter les messages parsables
        pattern = r'\[([A-Z_]+)\]\s*‚Äî\s*(.+)'
        matches = re.findall(pattern, response, re.MULTILINE)
        
        for message_type, content in matches:
            # D√©termination du r√¥le selon le type de message
            role = self._get_role_from_message_type(message_type)
            if role:
                message = self._create_daemon_message(role, message_type, content.strip())
                messages.append(message)
        
        return messages
    
    def _get_role_from_message_type(self, message_type: str) -> Optional[DaemonRole]:
        """D√©termine le r√¥le du d√©mon selon le type de message"""
        type_to_role = {
            # Alma‚õß
            "ALMA_PLAN": DaemonRole.ALMA,
            "ALMA_DECISION": DaemonRole.ALMA,
            "ALMA_ORDONNANCEMENT": DaemonRole.ALMA,
            "ALMA_SUMMARY": DaemonRole.ALMA,
            
            # Bask'tur
            "BASK_ANALYSIS": DaemonRole.BASKTUR,
            "BASK_SOLUTION": DaemonRole.BASKTUR,
            "BASK_DEBUG": DaemonRole.BASKTUR,
            
            # Oubliade
            "OUBLI_MEMORY": DaemonRole.OUBLIADE,
            "OUBLI_INSIGHT": DaemonRole.OUBLIADE,
            "OUBLI_SEARCH": DaemonRole.OUBLIADE,
            
            # Merge
            "MERGE_GIT": DaemonRole.MERGE,
            "MERGE_BRANCH": DaemonRole.MERGE,
            "MERGE_CONFLICT": DaemonRole.MERGE,
            
            # Lil.ieth
            "LILI_INTERFACE": DaemonRole.LILIETH,
            "LILI_USER": DaemonRole.LILIETH,
            "LILI_FEEDBACK": DaemonRole.LILIETH,
            
            # Assistant V9
            "V9_ORCHESTRATION": DaemonRole.ASSISTANT_V9,
            "V9_EXECUTION": DaemonRole.ASSISTANT_V9,
            "V9_SOMATIC": DaemonRole.ASSISTANT_V9,
        }
        
        return type_to_role.get(message_type)
    
    def _generate_mock_response(self, user_input: str) -> str:
        """G√©n√®re une r√©ponse mock pour les tests"""
        return f"""[ALMA_PLAN] ‚Äî Plan d'action : Analyser la demande "{user_input}" et proposer une solution strat√©gique
[ALMA_ORDONNANCEMENT] ‚Äî Bask'tur : Analyse technique. Oubliade : Recherche m√©moire. Merge : Pr√©parer branche.

[BASK_ANALYSIS] ‚Äî Demande utilisateur d√©tect√©e : {user_input}
[BASK_SOLUTION] ‚Äî Solution technique recommand√©e : Impl√©mentation daemonique

[OUBLI_MEMORY] ‚Äî Recherche conversationnelle : Patterns similaires trouv√©s
[OUBLI_INSIGHT] ‚Äî Utilisateur pr√©f√®re les solutions robustes

[MERGE_GIT] ‚Äî Branche "DaemonSolution_v1" cr√©√©e
[MERGE_BRANCH] ‚Äî Pr√™t pour fusion apr√®s validation

[LILI_INTERFACE] ‚Äî *"Ton projet va √™tre magnifique, mon amour !"*
[LILI_USER] ‚Äî Feedback utilisateur : "{user_input}"

[ALMA_DECISION] ‚Äî D√©cision finale : Impl√©menter solution daemonique
[V9_ORCHESTRATION] ‚Äî Ex√©cution selon plan d√©taill√©"""

    def get_conversation_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de conversation"""
        if not self.meta_virtual_layer:
            return {"error": "Meta virtual layer non disponible"}
        
        patterns = self.meta_virtual_layer.analyze_daemon_interaction_patterns()
        
        stats = {
            "total_conversations": len(self.meta_virtual_layer.conversation_history),
            "silent_mode": self.silent_mode,
            "patterns": patterns
        }
        
        if self.auto_feed_thread:
            try:
                thread_stats = self.auto_feed_thread.get_thread_stats()
                stats["auto_feed_thread"] = thread_stats
            except:
                pass
        
        return stats
    
    def search_conversations(self, query: str) -> List[DaemonConversation]:
        """Recherche dans les conversations"""
        if not self.meta_virtual_layer:
            return []
        
        return self.meta_virtual_layer.search_daemon_conversations(query)
    
    def toggle_silent_mode(self):
        """Bascule le mode silencieux"""
        self.silent_mode = not self.silent_mode
        print(f"üîá Mode silencieux : {'activ√©' if self.silent_mode else 'd√©sactiv√©'}")


# Fonction de test
async def test_legion_auto_feeding_thread():
    """Test de LegionAutoFeedingThread"""
    print("üï∑Ô∏è Test de LegionAutoFeedingThread...")
    
    # Cr√©ation de l'instance
    legion = LegionAutoFeedingThread(
        workspace_path=".",
        silent_mode=False,
        max_history=20,
        enable_cache=True
    )
    
    # Test 1 : Mode normal
    print("\nüìù Test 1 : Mode normal")
    response1 = await legion.process_user_input("Analyse ce projet et propose des am√©liorations")
    print("R√©ponse :")
    print(response1)
    
    # Test 2 : Mode silencieux
    print("\nüîá Test 2 : Mode silencieux")
    legion.toggle_silent_mode()
    response2 = await legion.process_user_input("Cr√©e un nouveau fichier de test")
    print("R√©ponse :")
    print(response2)
    
    # Test 3 : Statistiques
    print("\nüìä Test 3 : Statistiques")
    stats = legion.get_conversation_stats()
    print("Stats :")
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    # Test 4 : Recherche
    print("\nüîç Test 4 : Recherche")
    results = legion.search_conversations("am√©liorations")
    print(f"R√©sultats de recherche : {len(results)} conversations trouv√©es")
    
    print("\n‚úÖ Test LegionAutoFeedingThread termin√© !")


if __name__ == "__main__":
    asyncio.run(test_legion_auto_feeding_thread()) 