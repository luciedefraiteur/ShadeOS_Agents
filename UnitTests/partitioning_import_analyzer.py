#!/usr/bin/env python3
"""
Analyseur d'imports utilisant le partitioner pour une analyse prÃ©cise
des dÃ©pendances des auto-feeding threads.
"""

import os
import sys
import argparse
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Set, List, Dict, Optional, Tuple
from collections import defaultdict, deque

# Ajouter le chemin pour importer le partitioner
sys.path.append('Assistants/EditingSession')

# Ajouter le rÃ©pertoire racine pour les imports Core
sys.path.append('.')

try:
    from partitioning.ast_partitioners import PythonASTPartitioner
    from partitioning.partition_schemas import PartitioningError
    print("âœ… Partitioner importÃ© avec succÃ¨s!")
except ImportError as e:
    print(f"âŒ Erreur import partitioner: {e}")
    sys.exit(1)

try:
    from Core.LoggingProviders.base_logging_provider import BaseLoggingProvider
    from Core.LoggingProviders.console_logging_provider import ConsoleLoggingProvider
    from Core.LoggingProviders.file_logging_provider import FileLoggingProvider
    print("âœ… Providers de logging importÃ©s avec succÃ¨s!")
except ImportError as e:
    print(f"âŒ Erreur import providers: {e}")
    sys.exit(1)


class SimpleImportAnalyzerLogger:
    """Logger simple et direct pour l'analyse d'imports, sans dÃ©pendances complexes."""
    
    def __init__(self, log_directory: Optional[str] = None, log_format: str = "json"):
        self.log_directory = log_directory
        self.log_format = log_format
        self.session_id = f"analysis_{int(time.time())}"
        self.start_time = time.time()
        self.analysis_data = {
            'files_analyzed': [],
            'imports_found': {},
            'cycles_detected': [],
            'stats': {}
        }
        
        # CrÃ©er le rÃ©pertoire de logs si nÃ©cessaire
        if log_directory:
            Path(log_directory).mkdir(parents=True, exist_ok=True)
            # CrÃ©er le sous-rÃ©pertoire imports_analysis
            imports_analysis_dir = Path(log_directory) / "imports_analysis"
            imports_analysis_dir.mkdir(parents=True, exist_ok=True)
            
            # Fichier de log JSON
            self.log_file = imports_analysis_dir / "imports_analysis.log"
            
            # Fichier de rapport Markdown
            self.md_file = imports_analysis_dir / "imports_analysis_report.md"
            
            # Initialiser le fichier Markdown
            self._init_md_report()
        else:
            self.log_file = None
            self.md_file = None
    
    def _init_md_report(self):
        """Initialise le rapport Markdown avec l'en-tÃªte."""
        if self.md_file:
            with open(self.md_file, 'w', encoding='utf-8') as f:
                f.write(f"""# ğŸ“Š Rapport d'Analyse d'Imports

**Session ID:** `{self.session_id}`  
**Date:** {datetime.fromtimestamp(self.start_time).strftime('%Y-%m-%d %H:%M:%S')}  
**Format:** Analyse des dÃ©pendances Python avec dÃ©tection de cycles intelligente

---

## ğŸ¯ RÃ©sumÃ© ExÃ©cutif

*Ce rapport dÃ©taille l'analyse des imports Python dans le projet, incluant les dÃ©pendances locales, les cycles dÃ©tectÃ©s et les statistiques d'analyse.*

---

## ğŸ“ Fichiers AnalysÃ©s

""")
    
    def _write_md_section(self, content: str):
        """Ã‰crit une section dans le fichier Markdown."""
        if self.md_file:
            with open(self.md_file, 'a', encoding='utf-8') as f:
                f.write(content + "\n")
    
    def _write_log(self, level: str, message: str, data: Dict = None):
        """Ã‰crit un log dans le fichier."""
        if not self.log_file:
            return
            
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = {
            "timestamp": timestamp,
            "level": level,
            "session_id": self.session_id,
            "message": message
        }
        
        if data:
            log_entry.update(data)
        
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                if self.log_format == "json":
                    f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                else:
                    f.write(f"[{timestamp}] {level}: {message}\n")
        except Exception as e:
            print(f"âŒ Erreur Ã©criture log: {e}")
    
    def log_analysis_start(self, start_files: List[str]):
        """Log le dÃ©but d'une analyse."""
        self._write_log("INFO", "ğŸš€ DÃ©but analyse", {
            "type": "analysis_start",
            "start_files": start_files,
            "total_start_files": len(start_files)
        })
        print(f"ğŸš€ DÃ©but analyse session {self.session_id}")
        print(f"ğŸ“ Fichiers de dÃ©part: {len(start_files)}")
    
    def log_file_analysis_start(self, file_path: str, depth: int = 0):
        """Log le dÃ©but de l'analyse d'un fichier."""
        self._write_log("INFO", f"ğŸ“ Analyse: {file_path}", {
            "type": "file_analysis_start",
            "file_path": file_path,
            "depth": depth
        })
        indent = '  ' * depth
        print(f"{indent}ğŸ“ Analyse: {file_path}")
    
    def log_import_resolution(self, import_name: str, current_file: str, resolved_path: Optional[str], resolution_time: float):
        """Log la rÃ©solution d'un import."""
        self._write_log("INFO", f"Import rÃ©solu: {import_name}", {
            "type": "import_resolution",
            "import_name": import_name,
            "current_file": current_file,
            "resolved_path": resolved_path,
            "resolution_time": resolution_time,
            "success": resolved_path is not None
        })
        
        if resolved_path:
            print(f"  âœ… {import_name} -> {resolved_path}")
        else:
            print(f"  âŒ {import_name} -> Non rÃ©solu")
    
    def log_imports_summary(self, file_path: str, local_imports: List[str], standard_imports: List[str], third_party_imports: List[str]):
        """Log un rÃ©sumÃ© des imports d'un fichier."""
        self._write_log("INFO", f"RÃ©sumÃ© imports: {file_path}", {
            "type": "imports_summary",
            "file_path": file_path,
            "local_imports": local_imports,
            "standard_imports": standard_imports,
            "third_party_imports": third_party_imports,
            "total_imports": len(local_imports) + len(standard_imports) + len(third_party_imports)
        })
        print(f"ğŸ“Š RÃ©sumÃ© {file_path}: {len(local_imports)} locaux, {len(standard_imports)} standard, {len(third_party_imports)} tiers")
    
    def log_file_analysis_complete(self, file_path: str, resolved_count: int, total_imports: int):
        """Log la fin de l'analyse d'un fichier."""
        self._write_log("INFO", f"Analyse terminÃ©e: {file_path}", {
            "type": "file_analysis_complete",
            "file_path": file_path,
            "resolved_count": resolved_count,
            "total_imports": total_imports,
            "resolution_rate": resolved_count / total_imports if total_imports > 0 else 0.0
        })
        print(f"  ğŸ“¦ {file_path}: {resolved_count}/{total_imports} rÃ©solus")
    
    def log_recursive_analysis_complete(self, all_dependencies: Set[str], unused_files: Set[str]):
        """Log la fin de l'analyse rÃ©cursive."""
        total_time = time.time() - self.start_time
        self._write_log("INFO", "ğŸ¯ Analyse rÃ©cursive terminÃ©e", {
            "type": "recursive_analysis_complete",
            "all_dependencies": list(all_dependencies),
            "unused_files": list(unused_files),
            "total_dependencies": len(all_dependencies),
            "total_unused": len(unused_files),
            "total_time": total_time
        })
        print(f"ğŸ¯ Analyse terminÃ©e: {len(all_dependencies)} dÃ©pendances, {len(unused_files)} non utilisÃ©s")
        print(f"â±ï¸ Temps total: {total_time:.2f}s")
    
    def log_error(self, message: str, error: Exception = None):
        """Log une erreur."""
        error_data = {"type": "error", "message": message}
        if error:
            error_data["error_type"] = type(error).__name__
            error_data["error_details"] = str(error)
        
        self._write_log("ERROR", message, error_data)
        print(f"âŒ ERREUR: {message}")
        if error:
            print(f"   DÃ©tails: {error}")
    
    def log_warning(self, message: str, **kwargs):
        """Log un avertissement."""
        self._write_log("WARNING", message, {"type": "warning"})
        print(f"âš ï¸ {message}")
        if 'file' in kwargs:
            self._add_file_to_md_report(kwargs['file'], [], 0) # No imports for warning, depth 0
        elif 'cycle' in kwargs:
            self._add_cycle_to_md_report(kwargs['cycle'])
    
    def log_info(self, message: str, **kwargs):
        """Log un message d'information."""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
        log_entry = {
            'timestamp': timestamp,
            'level': 'INFO',
            'message': message,
            'session_id': self.session_id,
            **kwargs
        }
        
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        
        # Ajouter au rapport Markdown si c'est un message d'analyse
        if 'file' in kwargs:
            self._add_file_to_md_report(kwargs['file'], kwargs.get('imports', []), kwargs.get('depth', 0))
        elif 'cycle' in kwargs:
            self._add_cycle_to_md_report(kwargs['cycle'])
    
    def _add_file_to_md_report(self, file_path: str, imports: List[str], depth: int):
        """Ajoute un fichier analysÃ© au rapport Markdown."""
        indent = "  " * depth
        file_name = Path(file_path).name
        
        section = f"""
### {indent}ğŸ“„ {file_name}

**Chemin:** `{file_path}`  
**Profondeur:** {depth}

**Imports trouvÃ©s ({len(imports)}):**
"""
        
        if imports:
            for imp in imports:
                section += f"- `{imp}`\n"
        else:
            section += "- *Aucun import local trouvÃ©*\n"
        
        self._write_md_section(section)
    
    def _add_cycle_to_md_report(self, cycle: List[str]):
        """Ajoute un cycle dÃ©tectÃ© au rapport Markdown."""
        section = f"""
### âš ï¸ Cycle DÃ©tectÃ©

**Fichiers impliquÃ©s:**
"""
        for i, file_path in enumerate(cycle, 1):
            file_name = Path(file_path).name
            section += f"{i}. `{file_path}` ({file_name})\n"
        
        self._write_md_section(section)
    
    def log_debug(self, message: str, **kwargs):
        """Log un message de debug."""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
        log_entry = {
            'timestamp': timestamp,
            'level': 'DEBUG',
            'message': message,
            'session_id': self.session_id,
            **kwargs
        }
        
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def log_structured(self, level: str, message: str, **kwargs):
        """Log un message structurÃ©."""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
        log_entry = {
            'timestamp': timestamp,
            'level': level.upper(),
            'message': message,
            'session_id': self.session_id,
            **kwargs
        }
        
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def finalize_report(self, stats: Dict):
        """Finalise le rapport Markdown avec les statistiques."""
        if not self.md_file:
            return
        
        end_time = time.time()
        duration = end_time - self.start_time
        
        summary = f"""
---

## ğŸ“ˆ Statistiques d'Analyse

**DurÃ©e totale:** {duration:.2f} secondes  
**Fichiers analysÃ©s:** {stats.get('files_analyzed', 0)}  
**Imports locaux trouvÃ©s:** {stats.get('local_imports', 0)}  
**Cycles dÃ©tectÃ©s:** {stats.get('cycles_detected', 0)}  
**Profondeur maximale:** {stats.get('max_depth', 0)}

### ğŸ” DÃ©tails par Type d'Import

"""
        
        import_types = stats.get('import_types', {})
        for import_type, count in import_types.items():
            summary += f"- **{import_type}:** {count}\n"
        
        summary += f"""
---

## ğŸ¯ Recommandations

"""
        
        if stats.get('cycles_detected', 0) > 0:
            summary += """
### âš ï¸ Cycles DÃ©tectÃ©s
Des cycles de dÃ©pendances ont Ã©tÃ© dÃ©tectÃ©s. ConsidÃ©rez:
- Refactoriser les imports circulaires
- Utiliser des imports conditionnels
- SÃ©parer les responsabilitÃ©s des modules
"""
        else:
            summary += """
### âœ… Aucun Cycle DÃ©tectÃ©
La structure des imports est saine, aucune action requise.
"""
        
        summary += f"""
---

*Rapport gÃ©nÃ©rÃ© automatiquement le {datetime.now().strftime('%Y-%m-%d Ã  %H:%M:%S')}*
"""
        
        self._write_md_section(summary)


class DependencyGraph:
    """Graphe de dÃ©pendances avec dÃ©tection de cycles intelligente."""
    
    def __init__(self):
        self.dependencies = defaultdict(set)
        self.visited = set()
        self.recursion_stack = set()
    
    def add_dependency(self, file_path: str, dependency: str):
        """Ajoute une dÃ©pendance si elle ne crÃ©e pas de cycle."""
        if not self._would_create_cycle(file_path, dependency):
            self.dependencies[file_path].add(dependency)
    
    def detect_cycles(self) -> List[List[str]]:
        """DÃ©tecte tous les cycles dans le graphe."""
        self.visited.clear()
        self.recursion_stack.clear()
        cycles = []
        
        for node in self.dependencies:
            if node not in self.visited:
                self._dfs_cycle_detection(node, [], cycles)
        
        return cycles
    
    def _dfs_cycle_detection(self, node: str, path: List[str], cycles: List[List[str]]):
        """DFS pour dÃ©tecter les cycles."""
        if node in self.recursion_stack:
            # Cycle dÃ©tectÃ©
            cycle_start = path.index(node)
            cycle = path[cycle_start:] + [node]
            cycles.append(cycle)
            return
        
        if node in self.visited:
            return
        
        self.visited.add(node)
        self.recursion_stack.add(node)
        path.append(node)
        
        for neighbor in self.dependencies[node]:
            self._dfs_cycle_detection(neighbor, path, cycles)
        
        path.pop()
        self.recursion_stack.remove(node)
    
    def get_cycle_free_dependencies(self, file_path: str) -> Set[str]:
        """Retourne les dÃ©pendances qui ne crÃ©ent pas de cycles."""
        safe_deps = set()
        for dep in self.dependencies[file_path]:
            if not self._would_create_cycle(file_path, dep):
                safe_deps.add(dep)
        return safe_deps
    
    def _would_create_cycle(self, from_file: str, to_file: str) -> bool:
        """VÃ©rifie si ajouter une dÃ©pendance crÃ©erait un cycle."""
        if from_file == to_file:
            return True
        
        # BFS pour vÃ©rifier s'il existe un chemin de to_file vers from_file
        queue = deque([to_file])
        visited = {to_file}
        
        while queue:
            current = queue.popleft()
            
            for neighbor in self.dependencies[current]:
                if neighbor == from_file:
                    return True  # Cycle dÃ©tectÃ©
                
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append(neighbor)
        
        return False
    
    def get_dependency_stats(self) -> Dict[str, any]:
        """Retourne des statistiques sur le graphe de dÃ©pendances."""
        total_files = len(self.dependencies)
        total_deps = sum(len(deps) for deps in self.dependencies.values())
        
        # Top 5 fichiers avec le plus de dÃ©pendances
        file_dep_counts = [(file, len(deps)) for file, deps in self.dependencies.items()]
        file_dep_counts.sort(key=lambda x: x[1], reverse=True)
        top_files = file_dep_counts[:5]
        
        return {
            'total_files': total_files,
            'total_dependencies': total_deps,
            'top_files': top_files
        }

class PartitioningImportAnalyzer:
    """Analyseur d'imports utilisant le partitioner pour une analyse prÃ©cise des dÃ©pendances."""
    
    def __init__(self, project_root: str = '.', logging_provider: BaseLoggingProvider = None):
        self.project_root = project_root
        self.partitioner = PythonASTPartitioner()
        self.import_resolver = None
        self.visited = set()
        self.all_dependencies = set()  # Ajout de l'attribut manquant
        self.dependency_graph = DependencyGraph()
        
        # Utiliser le logger simple par dÃ©faut
        if logging_provider is None:
            self.logger = SimpleImportAnalyzerLogger()
        else:
            # Si un provider est fourni, crÃ©er un logger simple qui l'utilise
            self.logger = SimpleImportAnalyzerLogger()
            # TODO: Adapter pour utiliser le provider si nÃ©cessaire
    
    def _get_import_resolver(self, current_file: str = None):
        """Retourne l'ImportResolver, en le crÃ©ant si nÃ©cessaire."""
        if self.import_resolver is None:
            try:
                from partitioning.import_resolver import ImportResolver
                # Utiliser le fichier actuel ou un fichier par dÃ©faut
                default_file = current_file or 'UnitTests/partitioning_import_analyzer.py'
                self.import_resolver = ImportResolver(project_root=self.project_root, current_file=default_file)
            except ImportError as e:
                self.logger.log_error(f"Impossible d'importer ImportResolver: {e}")
                return None
        return self.import_resolver
    
    def _safe_log(self, method_name: str, *args, **kwargs):
        """Appelle une mÃ©thode de logging de maniÃ¨re sÃ©curisÃ©e."""
        if hasattr(self.logger, method_name):
            try:
                method = getattr(self.logger, method_name)
                method(*args, **kwargs)
            except Exception as e:
                # Fallback vers les mÃ©thodes de base
                self.logger.log_error(f"Erreur dans {method_name}: {e}")
        else:
            # Fallback vers les mÃ©thodes de base seulement si la mÃ©thode spÃ©cialisÃ©e n'existe pas
            if method_name == 'log_file_analysis_start':
                self.logger.log_info(f"ğŸ“ Analyse: {args[0] if args else 'Unknown'}")
            elif method_name == 'log_import_resolution':
                import_name, file_path, resolved_path = args[:3]
                if resolved_path:
                    self.logger.log_info(f"  âœ… {import_name} -> {resolved_path}")
                else:
                    self.logger.log_warning(f"  âŒ {import_name} -> Non rÃ©solu")
            elif method_name == 'log_imports_summary':
                file_path = args[0]
                local_imports, standard_imports, third_party_imports = args[1:4]
                self.logger.log_info(f"ğŸ“Š RÃ©sumÃ© {file_path}: {len(local_imports)} locaux, {len(standard_imports)} standard, {len(third_party_imports)} tiers")
            elif method_name == 'log_file_analysis_complete':
                file_path, resolved_count, total_imports = args[:3]
                self.logger.log_info(f"  ğŸ“¦ {file_path}: {resolved_count}/{total_imports} rÃ©solus")
            elif method_name == 'log_analysis_start':
                start_files = args[0] if args else []
                self.logger.log_info(f"ğŸš€ DÃ©but analyse avec {len(start_files)} fichiers")
            elif method_name == 'log_recursive_analysis_complete':
                all_dependencies, unused_files = args[:2]
                self.logger.log_info(f"ğŸ¯ Analyse terminÃ©e: {len(all_dependencies)} dÃ©pendances, {len(unused_files)} non utilisÃ©s")
    
    def extract_imports_with_partitioner(self, file_path: str) -> List[str]:
        """Extrait tous les imports d'un fichier Python avec le partitioner."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parser avec le partitioner
            tree = self.partitioner.parse_content(content, file_path)
            
            # Analyser les imports
            import_analysis = self.partitioner.extract_import_analysis(tree)
            
            # Utiliser la nouvelle liste unifiÃ©e
            if 'all_imports' in import_analysis:
                return import_analysis['all_imports']
            
            # Fallback sur l'ancienne mÃ©thode si pas de all_imports
            all_imports = []
            
            # Imports de la bibliothÃ¨que standard
            for import_info in import_analysis.get('standard_library', []):
                if isinstance(import_info, str):
                    all_imports.append(import_info)
                elif isinstance(import_info, dict):
                    module = import_info['module']
                    for name in import_info['names']:
                        all_imports.append(f"{module}.{name}")
            
            # Imports tiers
            for import_info in import_analysis.get('third_party', []):
                if isinstance(import_info, str):
                    all_imports.append(import_info)
                elif isinstance(import_info, dict):
                    module = import_info['module']
                    for name in import_info['names']:
                        all_imports.append(f"{module}.{name}")
            
            # Imports relatifs
            for import_info in import_analysis.get('relative_imports', []):
                module = import_info['module']
                level = import_info['level']
                for name in import_info['names']:
                    # Construire l'import relatif
                    dots = '.' * level
                    if module:
                        all_imports.append(f"{dots}{module}.{name}")
                    else:
                        all_imports.append(f"{dots}{name}")
            
            return all_imports
            
        except Exception as e:
            self.logger.log_error(f'Erreur partitioner {file_path}: {e}')
            return []
    
    def find_file_for_import(self, import_name: str, current_file: str) -> Optional[str]:
        """Trouve le fichier correspondant Ã  un import en utilisant l'ImportResolver."""
        try:
            resolver = self._get_import_resolver(current_file)
            if resolver is None:
                return None
                
            # VÃ©rifier si c'est un import de bibliothÃ¨que standard
            if self._is_standard_library_import(import_name):
                return None  # Ne pas rÃ©soudre les imports standard
                
            return resolver.resolve_import(import_name, current_file)
        except Exception as e:
            self.logger.log_error(f'Erreur ImportResolver pour {import_name}: {e}')
            return None
    
    def _is_standard_library_import(self, import_name: str) -> bool:
        """VÃ©rifie si un import fait partie de la bibliothÃ¨que standard."""
        standard_modules = {
            'time', 'json', 'os', 'sys', 'pathlib', 'typing', 'dataclasses', 
            'collections', 'abc', 're', 'asyncio', 'enum', 'datetime', 'logging',
            'argparse', 'functools', 'itertools', 'contextlib', 'threading',
            'multiprocessing', 'subprocess', 'tempfile', 'shutil', 'glob',
            'fnmatch', 'pickle', 'copy', 'weakref', 'types', 'inspect',
            'traceback', 'warnings', 'unittest', 'doctest', 'pdb', 'profile',
            'cProfile', 'timeit', 'dis', 'pickletools', 'tabnanny', 'py_compile',
            'compileall', 'keyword', 'token', 'tokenize', 'ast', 'symtable',
            'code', 'codeop', 'zipimport', 'pkgutil', 'modulefinder', 'runpy',
            'importlib', 'importlib.util', 'importlib.machinery', 'importlib.abc'
        }
        
        # Extraire le module principal
        module_name = import_name.split('.')[0]
        return module_name in standard_modules
    
    def analyze_file_recursively(self, file_path: str, depth: int = 0, 
                               local_only: bool = False, verbose: bool = False, debug: bool = False):
        """Analyse rÃ©cursivement un fichier et ses dÃ©pendances avec dÃ©tection de cycles intelligente."""
        if file_path in self.visited:
            if verbose:
                self.logger.log_info(f'ğŸ“ DÃ©jÃ  visitÃ©: {file_path} (profondeur {depth})')
            return
        
        self.visited.add(file_path)
        self.all_dependencies.add(file_path)
        
        # Log du dÃ©but d'analyse du fichier
        self._safe_log('log_file_analysis_start', file_path, depth)
        
        imports = self.extract_imports_with_partitioner(file_path)
        indent = '  ' * depth
        
        if not local_only or verbose:
            print(f'{indent}ğŸ“ {file_path}')
            print(f'{indent}   Imports (partitioner): {imports}')
        
        resolved_count = 0
        local_imports = []
        standard_imports = []
        third_party_imports = []
        
        for import_name in imports:
            # Ã‰viter les imports de bibliothÃ¨que standard en mode local_only
            if local_only and self._is_standard_library_import(import_name):
                continue
                
            start_time = time.time()
            local_file = self.find_file_for_import(import_name, file_path)
            resolution_time = time.time() - start_time
            
            # Log de la rÃ©solution d'import
            self._safe_log('log_import_resolution', import_name, file_path, local_file, resolution_time)
            
            if local_file and os.path.exists(local_file):
                # Ajouter au graphe de dÃ©pendances
                self.dependency_graph.add_dependency(file_path, local_file)
                
                # VÃ©rifier si cette dÃ©pendance crÃ©erait un cycle
                if not self.dependency_graph._would_create_cycle(file_path, local_file):
                    resolved_count += 1
                    local_imports.append((import_name, local_file))
                    if not local_only or verbose:
                        print(f'{indent}   âœ… {import_name} -> {local_file}')
                    # RÃ©cursion seulement si pas de cycle
                    if local_file not in self.visited:
                        self.analyze_file_recursively(local_file, depth + 1, local_only, verbose, debug)
                else:
                    if verbose:
                        print(f'{indent}   ğŸ”„ Cycle Ã©vitÃ©: {import_name} -> {local_file}')
            else:
                if not local_only or verbose:
                    print(f'{indent}   âŒ {import_name} -> Non rÃ©solu')
        
        # Log du rÃ©sumÃ© des imports
        self._safe_log('log_imports_summary', file_path, 
                      [imp[0] for imp in local_imports], standard_imports, third_party_imports)
        
        if local_only and local_imports:
            print(f'{indent}ğŸ“ {file_path}')
            print(f'{indent}   Imports locaux:')
            for import_name, local_file in local_imports:
                print(f'{indent}   âœ… {import_name} -> {local_file}')
        
        # Log de la fin d'analyse du fichier
        self._safe_log('log_file_analysis_complete', file_path, resolved_count, len(imports))
        
        if resolved_count > 0 and (not local_only or verbose):
            print(f'{indent}   ğŸ“¦ RÃ©solus: {resolved_count}')
    
    def analyze_autofeeding_dependencies(self, local_only: bool = False, verbose: bool = False, debug: bool = False):
        """Analyse les dÃ©pendances des auto-feeding threads."""
        autofeeding_files = [
            'Assistants/Generalist/V9_AutoFeedingThreadAgent.py',
            'Daemons/DaemonTeam/LegionAutoFeedingThread.py',
            'Daemons/DaemonTeam/LegionAutoFeedingThread_v2.py',
            'Core/UniversalAutoFeedingThread/base_auto_feeding_thread.py',
            'Core/UniversalAutoFeedingThread/universal_auto_feeding_thread.py'
        ]
        
        # Filtrer seulement les fichiers qui existent
        existing_files = [f for f in autofeeding_files if os.path.exists(f)]
        if not existing_files:
            self.logger.log_warning("Aucun fichier d'auto-feeding thread trouvÃ©")
            print("âš ï¸ Aucun fichier d'auto-feeding thread trouvÃ©")
            return set(), set()
        
        # Log du dÃ©but d'analyse
        self._safe_log('log_analysis_start', existing_files)
        
        if local_only:
            print('ğŸ¯ Analyse RÃ‰CURSIVE - IMPORTS LOCAUX SEULEMENT...')
        else:
            print('ğŸ” Analyse RÃ‰CURSIVE avec PARTITIONER...')
        print('ğŸ”„ DÃ©tection de cycles intelligente activÃ©e')
        print('=' * 80)
        
        for file_path in existing_files:
            if not local_only:
                print(f'\nğŸ“ DÃ‰PART: {file_path}')
                print('-' * 60)
            
            # Log du dÃ©but d'analyse du fichier
            self.logger.log_info(f"ğŸ“ Analyse de {file_path}", file=file_path, depth=0)
            
            # Analyser les imports du fichier
            imports = self.extract_imports_with_partitioner(file_path)
            
            # Log des imports trouvÃ©s
            if imports:
                self.logger.log_info(f"âœ… Imports trouvÃ©s dans {file_path}", 
                                   file=file_path, 
                                   imports=imports,
                                   depth=0)
            else:
                self.logger.log_info(f"ğŸ“„ Aucun import local trouvÃ© dans {file_path}", 
                                   file=file_path, 
                                   imports=[],
                                   depth=0)
            
            # Analyser rÃ©cursivement
            self.analyze_file_recursively(file_path, local_only=local_only, verbose=verbose, debug=debug)
        
        # Analyser les cycles dÃ©tectÃ©s
        cycles = self.dependency_graph.detect_cycles()
        if cycles:
            print(f'\nğŸ”„ CYCLES DÃ‰TECTÃ‰S: {len(cycles)}')
            for i, cycle in enumerate(cycles, 1):
                print(f'  Cycle {i}: {" -> ".join(cycle)}')
        else:
            print(f'\nâœ… AUCUN CYCLE DÃ‰TECTÃ‰')
        
        # Statistiques du graphe
        stats = self.dependency_graph.get_dependency_stats()
        print(f'\nğŸ“Š STATISTIQUES DU GRAPHE:')
        print(f'  Fichiers analysÃ©s: {stats["total_files"]}')
        print(f'  DÃ©pendances totales: {stats["total_dependencies"]}')
        print(f'  Cycles dÃ©tectÃ©s: {len(cycles)}') # Use len(cycles) from dependency_graph
        
        if stats['top_files']:
            print(f'\nğŸ“ˆ TOP 5 FICHIERS AVEC LE PLUS DE DÃ‰PENDANCES:')
            for file, count in stats['top_files'][:5]:
                print(f'  {file}: {count} dÃ©pendances')
        
        if not local_only:
            print(f'\nğŸ“Š TOTAL DÃ‰PENDANCES: {len(self.all_dependencies)}')
            print('\nğŸ“‹ Liste complÃ¨te des dÃ©pendances:')
            for dep in sorted(self.all_dependencies):
                print(f'  {dep}')
            
            # Trouver tous les fichiers Python du projet
            all_py_files = set()
            for root, dirs, files in os.walk(self.project_root):
                dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', '.pytest_cache']]
                for file in files:
                    if file.endswith('.py'):
                        all_py_files.add(str(Path(root) / file))
            
            unused_files = all_py_files - self.all_dependencies
            print(f'\nğŸ—‘ï¸ FICHIERS NON UTILISÃ‰S: {len(unused_files)}')
            print('\nğŸ“‹ Premiers fichiers non utilisÃ©s:')
            for file_path in sorted(unused_files)[:20]:
                print(f'  {file_path}')
        else:
            print(f'\nğŸ“Š TOTAL FICHIERS AVEC IMPORTS LOCAUX: {len(self.all_dependencies)}')
        
        # Log de la fin d'analyse
        unused_files = set() if local_only else unused_files
        self._safe_log('log_recursive_analysis_complete', self.all_dependencies, unused_files)
        
        # Finaliser le rapport Markdown
        final_stats = {
            'files_analyzed': stats['total_files'],
            'local_imports': len(self.all_dependencies),
            'cycles_detected': len(cycles),
            'max_depth': stats.get('max_depth', 0),
            'duration': 0,  # Pas de durÃ©e calculÃ©e dans cette mÃ©thode
            'import_types': {}  # Pas de types d'imports calculÃ©s dans cette mÃ©thode
        }
        self.logger.finalize_report(final_stats)
        
        return self.all_dependencies, unused_files if not local_only else set()

    def analyze_imports(self, files_to_analyze: List[str]) -> Dict:
        """Analyse les imports des fichiers donnÃ©s avec dÃ©tection de cycles intelligente et analyse rÃ©cursive."""
        self.logger.log_info("ğŸš€ DÃ©but de l'analyse d'imports rÃ©cursive", 
                           files_count=len(files_to_analyze),
                           files=files_to_analyze)
        
        start_time = time.time()
        local_dependencies = set()  # Seulement les imports locaux
        all_dependencies = set()    # Tous les imports pour les stats
        import_types = defaultdict(int)
        max_depth = 0
        
        # RÃ©initialiser l'Ã©tat pour une nouvelle analyse
        self.visited.clear()
        self.dependency_graph = DependencyGraph()
        
        # Analyser chaque fichier de maniÃ¨re rÃ©cursive
        for file_path in files_to_analyze:
            if file_path in self.visited:
                continue
                
            self.logger.log_info(f"ğŸ“ Analyse rÃ©cursive de {file_path}", file=file_path, depth=0)
            
            # Utiliser la mÃ©thode rÃ©cursive existante
            self.analyze_file_recursively(
                file_path=file_path,
                depth=0,
                local_only=True,  # Suivre seulement les imports locaux
                verbose=False,
                debug=False
            )
        
        # Collecter toutes les dÃ©pendances trouvÃ©es et filtrer les locaux
        for file_path in self.visited:
            imports = self.extract_imports_with_partitioner(file_path)
            if imports:
                all_dependencies.update(imports)
                
                # Filtrer les imports locaux pour le rapport
                local_imports_for_file = []
                
                # Compter les types d'imports et filtrer les locaux
                for imp in imports:
                    if imp.startswith('.'):
                        import_types['relative'] += 1
                        local_dependencies.add(imp)
                        local_imports_for_file.append(imp)
                    elif (imp.startswith('Core.') or imp.startswith('Assistants.') or 
                          imp.startswith('UnitTests.') or imp.startswith('MemoryEngine.')):
                        import_types['local'] += 1
                        local_dependencies.add(imp)
                        local_imports_for_file.append(imp)
                    else:
                        import_types['external'] += 1
                        # Ne pas ajouter les imports externes
                
                # Ajouter seulement les imports locaux au rapport Markdown
                self.logger._add_file_to_md_report(file_path, local_imports_for_file, 0)
            else:
                # Ajouter le fichier mÃªme s'il n'a pas d'imports
                self.logger._add_file_to_md_report(file_path, [], 0)
        
        # DÃ©tecter les cycles
        cycles = self.dependency_graph.detect_cycles()
        for cycle in cycles:
            self.logger.log_warning(f"âš ï¸ Cycle dÃ©tectÃ©: {' -> '.join(cycle)}", cycle=cycle)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Statistiques finales
        stats = {
            'files_analyzed': len(self.visited),
            'local_imports': len(local_dependencies),  # Seulement les imports locaux
            'total_imports': len(all_dependencies),    # Tous les imports
            'cycles_detected': len(cycles),
            'max_depth': 0,  # Pour l'instant, on ne calcule pas la profondeur
            'duration': duration,
            'import_types': dict(import_types)
        }
        
        self.logger.log_info("ğŸ¯ Analyse rÃ©cursive terminÃ©e", 
                           stats=stats,
                           duration=duration)
        
        # Finaliser le rapport Markdown avec seulement les imports locaux
        self.logger.finalize_report(stats)
        
        return {
            'dependencies': list(local_dependencies),  # Retourner seulement les imports locaux
            'all_dependencies': list(all_dependencies),  # Tous les imports pour rÃ©fÃ©rence
            'cycles': cycles,
            'stats': stats
        }

def main():
    """Fonction principale du script d'analyse d'imports."""
    parser = argparse.ArgumentParser(description='Analyseur d\'imports avec partitioner')
    parser.add_argument('--local-only', action='store_true', help='Analyser seulement les imports locaux')
    parser.add_argument('--verbose', action='store_true', help='Mode verbeux')
    parser.add_argument('--debug', action='store_true', help='Mode debug')
    parser.add_argument('--show-cycles', action='store_true', help='Afficher les cycles dÃ©tectÃ©s')
    parser.add_argument('--log-output', action='store_true', help='Activer la sortie de logs')
    parser.add_argument('--log-directory', default='logs', help='RÃ©pertoire pour les logs')
    parser.add_argument('--log-format', choices=['json', 'text'], default='json', help='Format des logs')
    parser.add_argument('--use-import-analyzer-provider', action='store_true', help='Utiliser le provider spÃ©cialisÃ©')
    
    args = parser.parse_args()
    
    # DÃ©finir les variables
    log_directory = args.log_directory
    files_to_analyze = [
        'Assistants/Generalist/V9_AutoFeedingThreadAgent.py',
        'Daemons/DaemonTeam/LegionAutoFeedingThread.py',
        'Daemons/DaemonTeam/LegionAutoFeedingThread_v2.py',
        'Core/UniversalAutoFeedingThread/base_auto_feeding_thread.py',
        'Core/UniversalAutoFeedingThread/universal_auto_feeding_thread.py'
    ]
    
    # Configurer le logger simple
    if args.log_output:
        logger = SimpleImportAnalyzerLogger(
            log_directory=log_directory,
            log_format=args.log_format
        )
    else:
        logger = SimpleImportAnalyzerLogger()
    
    # CrÃ©er l'analyseur avec le logger simple
    analyzer = PartitioningImportAnalyzer(logging_provider=None)  # On utilise le logger simple intÃ©grÃ©
    # Remplacer le logger intÃ©grÃ© par celui configurÃ©
    analyzer.logger = logger
    
    print("ğŸ§  DÃ©tection de cycles intelligente (sans limite de profondeur)")
    print("=" * 60)
    
    # Analyser les dÃ©pendances
    print(f"ğŸ” Analyse des auto-feeding threads...")
    
    # Utiliser la mÃ©thode gÃ©nÃ©rique avec la liste des fichiers d'auto-feeding threads
    result = analyzer.analyze_imports(files_to_analyze)
    
    # Afficher les rÃ©sultats
    print(f"\nğŸ“Š RÃ©sultats de l'analyse des auto-feeding threads:")
    print(f"   Fichiers analysÃ©s: {result['stats']['files_analyzed']}")
    print(f"   Imports locaux trouvÃ©s: {result['stats']['local_imports']}")
    print(f"   Cycles dÃ©tectÃ©s: {result['stats']['cycles_detected']}")
    print(f"   DurÃ©e: {result['stats']['duration']:.2f}s")
    
    if result['dependencies']:
        print(f"\nğŸ“¦ DÃ©pendances trouvÃ©es:")
        for dep in sorted(result['dependencies']):
            print(f"   - {dep}")
    
    if result['cycles']:
        print(f"\nâš ï¸ Cycles dÃ©tectÃ©s:")
        for cycle in result['cycles']:
            print(f"   {' -> '.join(cycle)}")
    else:
        print(f"\nâœ… Aucun cycle dÃ©tectÃ© - structure saine!")
    
    if args.show_cycles and result['cycles']:
        print(f"\nğŸ” DÃ©tails des cycles:")
        for i, cycle in enumerate(result['cycles'], 1):
            print(f"   Cycle {i}: {' -> '.join(cycle)}")
    
    print(f"\nğŸ“ Rapport dÃ©taillÃ© gÃ©nÃ©rÃ© dans: {log_directory}/imports_analysis/")
    print(f"   - Log JSON: imports_analysis.log")
    print(f"   - Rapport Markdown: imports_analysis_report.md")

if __name__ == '__main__':
    main() 